{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7855fc58",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'findspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfindspark\u001b[39;00m\n\u001b[1;32m      2\u001b[0m findspark\u001b[38;5;241m.\u001b[39minit()\n\u001b[1;32m      3\u001b[0m findspark\u001b[38;5;241m.\u001b[39mfind()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'findspark'"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c3a4f4",
   "metadata": {},
   "source": [
    "# Creamos uns Session de spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ecdd58",
   "metadata": {},
   "source": [
    "## SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76c5c1ac",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 18:27:47 WARN Utils: Your hostname, emi-All-Series resolves to a loopback address: 127.0.1.1; using 192.168.100.35 instead (on interface enp3s0)\n",
      "24/05/11 18:27:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/11 18:27:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.100.35:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>TaxiOperationsDataFrameApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3b37af4d90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "#from pyspark.sql.types import *\n",
    "#from pyspark.sql.functions import *\n",
    "\n",
    "spark = (\n",
    "            SparkSessionSmetana dies in Prague, Czech republic, 1884\n",
    "may 12 \tThe Cat Parade\n",
    "\n",
    "                .builder\n",
    "    \n",
    "                .appName(\"TaxiOperationsDataFrameApp\")\n",
    "                .master(\"local[4]\")\n",
    "    \n",
    "                .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "                .config(\"spark.sql.adaptive.enabled\", \"false\")    \n",
    "                .getOrCreate()\n",
    "        )\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eed3fd",
   "metadata": {},
   "source": [
    "El código crea una sesión de Spark llamada \"TaxiOperationsDataFrameApp\" con 4 núcleos locales y deshabilita la asignación dinámica y la optimización adaptativa de SQL.\n",
    "\n",
    "Explicación paso a paso del código:\n",
    "1. Importa las clases necesarias de pyspark.\n",
    "2. Crea una sesión de Spark llamada \"TaxiOperationsDataFrameApp\" con 4 núcleos locales.\n",
    "3. Deshabilita la asignación dinámica y la optimización adaptativa de SQL.\n",
    "4. Obtiene o crea la sesión de Spark.\n",
    "5. Obtiene el contexto de Spark.\n",
    "6. Imprime la sesión de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc54c228",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import *\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5730b25e",
   "metadata": {},
   "source": [
    "Este código ajusta el estilo de visualización de los elementos \"pre\" en un entorno de IPython.\n",
    "\n",
    "Explicación paso a paso del código:\n",
    "1. Se importa la función \"display\" del módulo \"IPython.display\".\n",
    "2. Se utiliza la función \"display\" para mostrar un HTML con un estilo específico.\n",
    "3. El estilo especificado en el HTML es para los elementos \"pre\" y se establece el atributo \"white-space\" como \"pre !important\", lo que indica que se respetará el espaciado en blanco en estos elementos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92baa3c9",
   "metadata": {},
   "source": [
    "## Create DataFrame - Option 1.a: From RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38ed35ee",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Create RDD\n",
    "\n",
    "data = [\n",
    "            [ 1, \"Neha\",  10000 ],\n",
    "            [ 2, \"Steve\", 20000 ],\n",
    "            [ 3, \"Kari\",  30000 ],\n",
    "            [ 4, \"Ivan\",  40000 ],\n",
    "            [ 5, \"Mohit\", 50000 ]\n",
    "       ]\n",
    "\n",
    "employeesRdd = sc.parallelize(data) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d422ee6",
   "metadata": {},
   "source": [
    "1. Se crea un RDD llamado employeesRdd utilizando la función sc.parallelize() que toma los datos como argumento. \n",
    "2. Los datos se distribuyen en el clúster para su procesamiento paralelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0b0513b",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataFrame and show content\n",
    "\n",
    "employeesDF = employeesRdd.toDF()\n",
    "type(employeesDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7256dbd1",
   "metadata": {},
   "source": [
    "1. Se llama al método toDF() en el RDD employeesRdd, que convierte el RDD en un DataFrame. \n",
    "2. El DataFrame resultante se asigna a la variable employeesDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e35c10d",
   "metadata": {},
   "source": [
    "La diferencia principal entre un RDD (Resilient Distributed Dataset) y un DataFrame en Spark es que un RDD es una colección distribuida de elementos que pueden ser operados en paralelo, mientras que un DataFrame es una estructura de datos tabular con filas y columnas que permite realizar operaciones similares a las de una base de datos relacional.\n",
    "\n",
    "Algunas diferencias clave entre RDD y DataFrame son:\n",
    "\n",
    "1. Tipos de datos: \n",
    ">Un RDD puede contener cualquier tipo de objeto, mientras que un DataFrame tiene una estructura tabular con esquema definido que facilita la manipulación de datos.\n",
    "\n",
    "2. Optimización de consultas: \n",
    ">Los DataFrames en Spark utilizan un optimizador de consultas Catalyst que permite optimizar las operaciones y mejorar el rendimiento de las consultas, mientras que los RDD no tienen esta optimización.\n",
    "\n",
    "3. APIs: \n",
    ">Los DataFrames proporcionan APIs de alto nivel que permiten realizar operaciones de forma más sencilla y eficiente en comparación con los RDD, que requieren más código para realizar las mismas operaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ef40c63",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+\n",
      "| _1|   _2|   _3|\n",
      "+---+-----+-----+\n",
      "|  1| Neha|10000|\n",
      "|  2|Steve|20000|\n",
      "|  3| Kari|30000|\n",
      "|  4| Ivan|40000|\n",
      "|  5|Mohit|50000|\n",
      "+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "employeesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb256ca3",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| Id| Name|Salary|\n",
      "+---+-----+------+\n",
      "|  1| Neha| 10000|\n",
      "|  2|Steve| 20000|\n",
      "|  3| Kari| 30000|\n",
      "|  4| Ivan| 40000|\n",
      "|  5|Mohit| 50000|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define nombres de coluimnas\n",
    "\n",
    "employeesDF = employeesDF.toDF(\"Id\", \"Name\", \"Salary\")\n",
    "\n",
    "employeesDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755fc604",
   "metadata": {},
   "source": [
    "Imprimimos el esquema de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01b2b9a7",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print DataFrame schema\n",
    "\n",
    "employeesDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e8bb0c",
   "metadata": {},
   "source": [
    "## Create DataFrame - Option 1.b: From data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "719e3e0d",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| Id| Name|Salary|\n",
      "+---+-----+------+\n",
      "|  1| Neha| 10000|\n",
      "|  2|Steve| 20000|\n",
      "|  3| Kari| 30000|\n",
      "|  4| Ivan| 40000|\n",
      "|  5|Mohit| 50000|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame from collection\n",
    "\n",
    "employeesDF = (\n",
    "                    spark\n",
    "                        .createDataFrame\n",
    "                        (\n",
    "                            data,                                   # Pass RDD or collection\n",
    "                            \"Id: long, Name: string, Salary: long\"  \n",
    "                            \n",
    "                                              # Pass schema as array [\"Id\", \"Name\", \"Salary\"]\n",
    "                        )\n",
    "               )\n",
    "\n",
    "employeesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb16be37",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301f64e7",
   "metadata": {},
   "source": [
    "## Create DataFrame - Option 2: Read a File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cf382df",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Read YellowTaxis csv file to create DataFrame\n",
    "\n",
    "yellowTaxiDF = (\n",
    "                  spark    \n",
    "                    .read    \n",
    "                    .csv(\"datos/YellowTaxis_202210.csv\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26174bbe",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+--------------------+------------+--------------------+-----------+\n",
      "|     _c0|                 _c1|                 _c2|            _c3|          _c4|       _c5|               _c6|         _c7|         _c8|         _c9|       _c10| _c11|   _c12|      _c13|        _c14|                _c15|        _c16|                _c17|       _c18|\n",
      "+--------+--------------------+--------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+--------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_date...|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surch...|total_amount|congestion_surcharge|airport_fee|\n",
      "|       1|2022-10-01T05:33:...|2022-10-01T05:48:...|            1.0|          1.7|       1.0|                 N|         249|         107|           1|        9.5|  3.0|    0.5|      2.65|         0.0|                 0.3|       15.95|                 2.5|        0.0|\n",
      "|       2|2022-10-01T05:44:...|2022-10-01T05:49:...|            2.0|         0.72|       1.0|                 N|         151|         238|           2|        5.5|  0.5|    0.5|       0.0|         0.0|                 0.3|         9.3|                 2.5|        0.0|\n",
      "|       2|2022-10-01T05:57:...|2022-10-01T06:07:...|            1.0|         1.74|       1.0|                 N|         238|         166|           1|        9.0|  0.5|    0.5|      2.06|         0.0|                 0.3|       12.36|                 0.0|        0.0|\n",
      "|       1|2022-10-01T06:02:...|2022-10-01T06:08:...|            0.0|          1.3|       1.0|                 N|         142|         239|           1|        6.5|  3.0|    0.5|      2.05|         0.0|                 0.3|       12.35|                 2.5|        0.0|\n",
      "+--------+--------------------+--------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+--------------------+------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display DataFrame content\n",
    "\n",
    "yellowTaxiDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c37bef1",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       1|2022-10-01T05:33:...| 2022-10-01T05:48:...|            1.0|          1.7|       1.0|                 N|         249|         107|           1|        9.5|  3.0|    0.5|      2.65|         0.0|                  0.3|       15.95|                 2.5|        0.0|\n",
      "|       2|2022-10-01T05:44:...| 2022-10-01T05:49:...|            2.0|         0.72|       1.0|                 N|         151|         238|           2|        5.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         9.3|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take column names from file header row\n",
    "\n",
    "yellowTaxiDF = (\n",
    "                  spark\n",
    "                    .read\n",
    "                    # toma la primer fila como nombres de columnas   \n",
    "                    .option(\"header\", \"true\")  \n",
    "    \n",
    "                    .csv(\"datos/YellowTaxis_202210.csv\")\n",
    "               )\n",
    "\n",
    "yellowTaxiDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "088d162a",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- RatecodeID: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- congestion_surcharge: string (nullable = true)\n",
      " |-- airport_fee: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yellowTaxiDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0d3aeb6",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorId|lpep_pickup_datetime|lpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2|2022-10-01T06:08:...| 2022-10-01T06:21:...|            1.0|         2.47|       1.0|                 N|         256|         225|         1.0|       11.5|  0.5|    0.5|      2.56|         0.0|                  0.3|       15.36|                 0.0|        0.0|\n",
      "|       2|2022-10-01T06:36:...| 2022-10-01T06:42:...|            1.0|         1.96|       1.0|                 N|         166|         152|         1.0|        7.5|  0.5|    0.5|       2.2|         0.0|                  0.3|        11.0|                 0.0|        0.0|\n",
      "|       2|2022-10-01T06:01:...| 2022-10-01T06:15:...|            1.0|         2.43|       1.0|                 N|          74|         262|         1.0|       11.5|  0.5|    0.5|       2.0|         0.0|                  0.3|       17.55|                2.75|        0.0|\n",
      "|       2|2022-10-01T06:21:...| 2022-10-01T06:28:...|            1.0|         2.25|       1.0|                 N|          74|          42|         1.0|        8.5|  0.5|    0.5|      1.96|         0.0|                  0.3|       11.76|                 0.0|        0.0|\n",
      "|       1|2022-10-01T06:12:...| 2022-10-01T06:24:...|            1.0|          0.0|       1.0|                 N|          74|         247|         1.0|       17.2|  0.0|    0.5|       0.0|         0.0|                  0.3|        18.0|                 0.0|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use tab delimiter to read GreenTaxis file\n",
    "\n",
    "greenTaxiDF = (\n",
    "                  spark\n",
    "                    .read                     \n",
    "                    .option(\"header\", \"true\")\n",
    "                    # establece el delimitador del archivo de datos como una tabulación. \n",
    "                    # Esto significa que los datos en el archivo estarán separados por tabulaciones en lugar de comas u otro carácter.    \n",
    "                    .option(\"delimiter\", \"\\t\")\n",
    "                    .csv(\"datos/GreenTaxis_*.csv\")\n",
    "              )\n",
    "\n",
    "greenTaxiDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a836370e",
   "metadata": {},
   "source": [
    "## Read JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6ba7a49",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|PaymentType|PaymentTypeID|\n",
      "+-----------+-------------+\n",
      "|Credit Card|            1|\n",
      "|       Cash|            2|\n",
      "|  No Charge|            3|\n",
      "|    Dispute|            4|\n",
      "|    Unknown|            5|\n",
      "|Voided Trip|            6|\n",
      "+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read PaymentTypes JSON file\n",
    "\n",
    "paymentTypesDF = (\n",
    "                      spark\n",
    "                        .read\n",
    "                        .json(\"datos/PaymentTypes.json\")\n",
    "                 )\n",
    "\n",
    "paymentTypesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "826362b5",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|PaymentType|PaymentTypeID|\n",
      "+-----------+-------------+\n",
      "|Credit Card|            1|\n",
      "|       Cash|            2|\n",
      "|  No Charge|            3|\n",
      "|    Dispute|            4|\n",
      "|    Unknown|            5|\n",
      "+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(spark.read.format(\"json\")\n",
    "                  .load(\"datos/PaymentTypes.json\")\n",
    "                  .show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923d5bc1",
   "metadata": {},
   "source": [
    "## Schema Option 1 - No schema inference or definition\n",
    "\n",
    "<i>Check for jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e37b6067",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- RatecodeID: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- congestion_surcharge: string (nullable = true)\n",
      " |-- airport_fee: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read YellowTaxis csv file to create DataFrame\n",
    "\n",
    "yellowTaxiDF = (\n",
    "                  spark\n",
    "                    .read    \n",
    "                    .option(\"header\", \"true\")    \n",
    "                    .csv(\"datos/YellowTaxis_202210.csv\")\n",
    "               )\n",
    "\n",
    "yellowTaxiDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f648bca4",
   "metadata": {},
   "source": [
    "## Schema Option 2 - Infer schema\n",
    "\n",
    "<i>Check for jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "882bd119",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read YellowTaxis csv file, and create DataFrame by inferring the schema\n",
    "\n",
    "yellowTaxiDF = (\n",
    "                  spark\n",
    "                    .read\n",
    "                    .option(\"header\", \"true\")\n",
    "#\"inferSchema\": Esta opción le dice a Spark que infiera automáticamente el esquema de los datos.    \n",
    "                    .option(\"inferSchema\", \"true\")\n",
    "                    .csv(\"datos/YellowTaxis_202210.csv\")\n",
    "               )\n",
    "\n",
    "yellowTaxiDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a087768",
   "metadata": {},
   "source": [
    "## Schema Option 3 - Define schema & apply\n",
    "\n",
    "<i>Check for jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae1e69c",
   "metadata": {},
   "source": [
    "### Tipos de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2bddf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType, DoubleType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fff48784",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdataType\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataType\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnullable\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmetadata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "A field in :class:`StructType`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "name : str\n",
      "    name of the field.\n",
      "dataType : :class:`DataType`\n",
      "    :class:`DataType` of the field.\n",
      "nullable : bool, optional\n",
      "    whether the field can be null (None) or not.\n",
      "metadata : dict, optional\n",
      "    a dict from string to simple type that can be toInternald to JSON automatically\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from pyspark.sql.types import StringType, StructField\n",
      ">>> (StructField(\"f1\", StringType(), True)\n",
      "...      == StructField(\"f1\", StringType(), True))\n",
      "True\n",
      ">>> (StructField(\"f1\", StringType(), True)\n",
      "...      == StructField(\"f2\", StringType(), True))\n",
      "False\n",
      "\u001b[0;31mFile:\u001b[0m           ~/Escritorio/curso_spark/env/lib/python3.8/site-packages/pyspark/sql/types.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "StructField?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2add4b",
   "metadata": {},
   "source": [
    "Tabla con los tipos de datos que Spark maneja comúnmente:\n",
    "\n",
    "| Tipo de Datos    | Descripción                           |\n",
    "|-------------------|---------------------------------------|\n",
    "| StringType        | Cadena de texto                       |\n",
    "| IntegerType       | Números enteros                       |\n",
    "| DoubleType        | Números de punto flotante de doble precisión |\n",
    "| FloatType         | Números de punto flotante de precisión simple |\n",
    "| LongType          | Números enteros largos                |\n",
    "| ShortType         | Números enteros cortos                |\n",
    "| ByteType          | Números enteros de 8 bits              |\n",
    "| BooleanType       | Valores booleanos (Verdadero/Falso)   |\n",
    "| DateType          | Fecha                                 |\n",
    "| TimestampType     | Marca de tiempo                       |\n",
    "\n",
    "En Spark, al definir un esquema para un DataFrame, se pueden especificar diferentes parámetros para cada campo, como el nombre del campo, el tipo de datos, si el campo puede ser nulo o no.\n",
    "\n",
    "Algunos de los parámetros que se pueden especificar al definir un campo en un esquema de Spark incluyen:\n",
    "\n",
    "- name : \n",
    "    \n",
    "    El nombre del campo en el esquema.\n",
    "\n",
    "- dataType :\n",
    "\n",
    "    El tipo de datos que se espera en el campo.\n",
    "\n",
    "- nullable  \n",
    "\n",
    "    Indica si el campo puede ser nulo o no.\n",
    "\n",
    "- metadata \n",
    "\n",
    "    Metadatos adicionales asociados con el campo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b16794b7",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Create schema for Yellow Taxi Data\n",
    "\n",
    "yellowTaxiSchema = (\n",
    "                        StructType\n",
    "                        ([              \n",
    "                            StructField( \n",
    "                                        name     = \"VendorId\", \n",
    "                                        dataType = IntegerType(), \n",
    "                                        nullable = True \n",
    "                                        ),\n",
    "                                        #   name,                    dataType,    nullable\n",
    "                            StructField(\"tpep_pickup_datetime\"   , TimestampType() , True),\n",
    "                            StructField(\"tpep_dropoff_datetime\"  , TimestampType() , True),\n",
    "                            StructField(\"passenger_count\"        , DoubleType()    , True),\n",
    "                            StructField(\"trip_distance\"          , DoubleType()    , True),\n",
    "                            StructField(\"RatecodeID\"             , DoubleType()    , True),\n",
    "                            StructField(\"store_and_fwd_flag\"     , StringType()    , True),\n",
    "                            StructField(\"PULocationID\"           , IntegerType()   , True),\n",
    "                            StructField(\"DOLocationID\"           , IntegerType()   , True),\n",
    "                            StructField(\"payment_type\"           , IntegerType()   , True),\n",
    "                            StructField(\"fare_amount\"            , DoubleType()    , True),\n",
    "                            StructField(\"extra\"                  , DoubleType()    , True),\n",
    "                            StructField(\"mta_tax\"                , DoubleType()    , True),\n",
    "                            StructField(\"tip_amount\"             , DoubleType()    , True),\n",
    "                            StructField(\"tolls_amount\"           , DoubleType()    , True),\n",
    "                            StructField(\"improvement_surcharge\"  , DoubleType()    , True),\n",
    "                            StructField(\"total_amount\"           , DoubleType()    , True),\n",
    "                            StructField(\"congestion_surcharge\"   , DoubleType()    , True),\n",
    "                            StructField(\"airport_fee\"            , DoubleType()    , True)\n",
    "                        ])\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4807c4f",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorId: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read YellowTaxis csv file, and create DataFrame by applying the schema\n",
    "\n",
    "yellowTaxiDF = (\n",
    "                  spark\n",
    "                    .read\n",
    "                    .option(\"header\", \"true\") # carga la cabezera como nombre de columnas\n",
    "                    # carga el esquema de tipo de datos \n",
    "                    .schema(yellowTaxiSchema)    \n",
    "                    .csv(\"datos/YellowTaxis_202210.csv\")\n",
    "               )\n",
    "\n",
    "yellowTaxiDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bced448f",
   "metadata": {},
   "source": [
    "### Define schema for JSON file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661015fa",
   "metadata": {},
   "source": [
    "Si cargamos un json que tiene en algun campo una estructura de datos anidada , el metodo reas.json dara error si no especificamos que es  multiliea "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe8e53d3",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Read JSON file\u001b[39;00m\n\u001b[1;32m      3\u001b[0m taxiBasesDF \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      4\u001b[0m                   spark\n\u001b[1;32m      5\u001b[0m                     \u001b[38;5;241m.\u001b[39mread\n\u001b[1;32m      6\u001b[0m                     \u001b[38;5;241m.\u001b[39mjson(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatos/TaxiBases.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m               )\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtaxiBasesDF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Escritorio/curso_spark/env/lib/python3.8/site-packages/pyspark/sql/dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Escritorio/curso_spark/env/lib/python3.8/site-packages/pyspark/sql/dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    960\u001b[0m     )\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Escritorio/curso_spark/env/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Escritorio/curso_spark/env/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count()."
     ]
    }
   ],
   "source": [
    "# Read JSON file\n",
    "\n",
    "taxiBasesDF = (\n",
    "                  spark\n",
    "                    .read\n",
    "                    .json(\"datos/TaxiBases.json\")\n",
    "              )\n",
    "\n",
    "taxiBasesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae93dc83",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+----------+--------------------------------------+------------------------------------------------+--------------+------------+----------------+--------+---------------------------+\n",
      "|Address                                     |Date      |Entity Name                           |GeoLocation                                     |License Number|SHL Endorsed|Telephone Number|Time    |Type of Base               |\n",
      "+--------------------------------------------+----------+--------------------------------------+------------------------------------------------+--------------+------------+----------------+--------+---------------------------+\n",
      "|{636, NEW YORK, 10001, NY, WEST   28 STREET}|08/15/2019|VIER-NY,LLC                           |{40.75273, (40.75273, -74.006408), -74.006408}  |B02865        |No          |6466657536      |18:03:31|BLACK CAR BASE             |\n",
      "|{131, BRONX, 10468, NY, KINGSBRIDGE ROAD}   |08/15/2019|VETERANS RADIO DISPATCHER CORP.       |{40.86927, (40.86927, -73.90281), -73.90281}    |B02634        |No          |7183647878      |18:03:31|LIVERY BASE                |\n",
      "|{115-54, ELMONT, 11003, NY, 238 STREET}     |08/15/2019|ALPHA VAN LINE                        |{40.693473, (40.693473, -73.724446), -73.724446}|B80094        |No          |5162850750      |18:03:31|COMMUTER VAN AUTHORITY BASE|\n",
      "|{866, BROOKLYN, 11208, NY, NEW LOTS AVENUE} |08/15/2019|A.T.B. CAR AND LIMOUSINE SERVICE, INC.|{40.667838, (40.667838, -73.8788), -73.8788}    |B02677        |No          |7184854444      |18:03:31|LIVERY BASE                |\n",
      "|{57-48, MASPETH, 11378, NY, MASPETH AVENUE} |08/15/2019|KYOEI LIMOUSINE, INC.                 |{40.722961, (40.722961, -73.91031), -73.91031}  |B02152        |No          |7183263258      |18:03:31|LUXURY/LIMOUSINE           |\n",
      "+--------------------------------------------+----------+--------------------------------------+------------------------------------------------+--------------+------------+----------------+--------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read multiline JSON file\n",
    "\n",
    "taxiBasesDF = (spark.read\n",
    "                   # Especificamos que el json tiene datos anidados\n",
    "                  .option(\"multiline\", \"true\")\n",
    "    \n",
    "                  .json(\"datos/TaxiBases.json\")\n",
    "              )\n",
    "\n",
    "taxiBasesDF.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3208b461",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+--------------------+--------------+------------+----------------+--------+--------------------+\n",
      "|             Address|      Date|         Entity Name|         GeoLocation|License Number|SHL Endorsed|Telephone Number|    Time|        Type of Base|\n",
      "+--------------------+----------+--------------------+--------------------+--------------+------------+----------------+--------+--------------------+\n",
      "|{636, NEW YORK, 1...|08/15/2019|         VIER-NY,LLC|{40.75273, (40.75...|        B02865|          No|      6466657536|18:03:31|      BLACK CAR BASE|\n",
      "|{131, BRONX, 1046...|08/15/2019|VETERANS RADIO DI...|{40.86927, (40.86...|        B02634|          No|      7183647878|18:03:31|         LIVERY BASE|\n",
      "|{115-54, ELMONT, ...|08/15/2019|      ALPHA VAN LINE|{40.693473, (40.6...|        B80094|          No|      5162850750|18:03:31|COMMUTER VAN AUTH...|\n",
      "|{866, BROOKLYN, 1...|08/15/2019|A.T.B. CAR AND LI...|{40.667838, (40.6...|        B02677|          No|      7184854444|18:03:31|         LIVERY BASE|\n",
      "|{57-48, MASPETH, ...|08/15/2019|KYOEI LIMOUSINE, ...|{40.722961, (40.7...|        B02152|          No|      7183263258|18:03:31|    LUXURY/LIMOUSINE|\n",
      "+--------------------+----------+--------------------+--------------------+--------------+------------+----------------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxiBasesDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c40ad37",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Address: struct (nullable = true)\n",
      " |    |-- Building: string (nullable = true)\n",
      " |    |-- City: string (nullable = true)\n",
      " |    |-- Postcode: long (nullable = true)\n",
      " |    |-- State: string (nullable = true)\n",
      " |    |-- Street: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Entity Name: string (nullable = true)\n",
      " |-- GeoLocation: struct (nullable = true)\n",
      " |    |-- Latitude: double (nullable = true)\n",
      " |    |-- Location: string (nullable = true)\n",
      " |    |-- Longitude: double (nullable = true)\n",
      " |-- License Number: string (nullable = true)\n",
      " |-- SHL Endorsed: string (nullable = true)\n",
      " |-- Telephone Number: long (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Type of Base: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the complex structure of JSON data\n",
    "\n",
    "taxiBasesDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3901294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "293b7d98",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Define schema for JSON data\n",
    "\n",
    "taxiBasesSchema = (\n",
    "                    StructType\n",
    "                    ([\n",
    "                        StructField(\"License Number\"         , StringType()    , True),\n",
    "                        StructField(\"Entity Name\"            , StringType()    , True),\n",
    "                        StructField(\"Telephone Number\"       , LongType()      , True),\n",
    "                        StructField(\"SHL Endorsed\"           , StringType()    , True),\n",
    "                        StructField(\"Type of Base\"           , StringType()    , True),\n",
    "\n",
    "                        StructField(\"Address\", \n",
    "                                        StructType\n",
    "                                        ([\n",
    "                                            StructField(\"Building\"   , StringType(),   True),\n",
    "                                            StructField(\"Street\"     , StringType(),   True), \n",
    "                                            StructField(\"City\"       , StringType(),   True), \n",
    "                                            StructField(\"State\"      , StringType(),   True), \n",
    "                                            StructField(\"Postcode\"   , StringType(),   True)\n",
    "                                        ]),\n",
    "                                    True\n",
    "                                   ),\n",
    "                        \n",
    "                        StructField(\"GeoLocation\", \n",
    "                                        StructType\n",
    "                                        ([\n",
    "                                            StructField(\"Latitude\"   , StringType(),   True),\n",
    "                                            StructField(\"Longitude\"  , StringType(),   True), \n",
    "                                            StructField(\"Location\"   , StringType(),   True)\n",
    "                                        ]),\n",
    "                                    True\n",
    "                                   )  \n",
    "                  ])\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41863f24",
   "metadata": {
    "metadata": {},
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------------------------+----------------+------------+---------------------------+--------------------------------------------+------------------------------------------------+\n",
      "|License Number|Entity Name                           |Telephone Number|SHL Endorsed|Type of Base               |Address                                     |GeoLocation                                     |\n",
      "+--------------+--------------------------------------+----------------+------------+---------------------------+--------------------------------------------+------------------------------------------------+\n",
      "|B02865        |VIER-NY,LLC                           |6466657536      |No          |BLACK CAR BASE             |{636, WEST   28 STREET, NEW YORK, NY, 10001}|{40.75273, -74.006408, (40.75273, -74.006408)}  |\n",
      "|B02634        |VETERANS RADIO DISPATCHER CORP.       |7183647878      |No          |LIVERY BASE                |{131, KINGSBRIDGE ROAD, BRONX, NY, 10468}   |{40.86927, -73.90281, (40.86927, -73.90281)}    |\n",
      "|B80094        |ALPHA VAN LINE                        |5162850750      |No          |COMMUTER VAN AUTHORITY BASE|{115-54, 238 STREET, ELMONT, NY, 11003}     |{40.693473, -73.724446, (40.693473, -73.724446)}|\n",
      "|B02677        |A.T.B. CAR AND LIMOUSINE SERVICE, INC.|7184854444      |No          |LIVERY BASE                |{866, NEW LOTS AVENUE, BROOKLYN, NY, 11208} |{40.667838, -73.8788, (40.667838, -73.8788)}    |\n",
      "|B02152        |KYOEI LIMOUSINE, INC.                 |7183263258      |No          |LUXURY/LIMOUSINE           |{57-48, MASPETH AVENUE, MASPETH, NY, 11378} |{40.722961, -73.91031, (40.722961, -73.91031)}  |\n",
      "+--------------+--------------------------------------+----------------+------------+---------------------------+--------------------------------------------+------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read JSON file by applying nested schema\n",
    "\n",
    "taxiBasesDF = (\n",
    "                  spark\n",
    "                    .read    \n",
    "                    .option(\"multiline\", \"true\")\n",
    "    \n",
    "                    .schema(taxiBasesSchema)\n",
    "    \n",
    "                    .json(\"datos/TaxiBases.json\")\n",
    "              )\n",
    "\n",
    "taxiBasesDF.show(5 ,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f13397d0",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- License Number: string (nullable = true)\n",
      " |-- Entity Name: string (nullable = true)\n",
      " |-- Telephone Number: long (nullable = true)\n",
      " |-- SHL Endorsed: string (nullable = true)\n",
      " |-- Type of Base: string (nullable = true)\n",
      " |-- Address: struct (nullable = true)\n",
      " |    |-- Building: string (nullable = true)\n",
      " |    |-- Street: string (nullable = true)\n",
      " |    |-- City: string (nullable = true)\n",
      " |    |-- State: string (nullable = true)\n",
      " |    |-- Postcode: string (nullable = true)\n",
      " |-- GeoLocation: struct (nullable = true)\n",
      " |    |-- Latitude: string (nullable = true)\n",
      " |    |-- Longitude: string (nullable = true)\n",
      " |    |-- Location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxiBasesDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0634686f",
   "metadata": {},
   "source": [
    "## Analyze Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afed9402",
   "metadata": {},
   "source": [
    "### Describe (datos estadisticos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "800f34c6",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+\n",
      "|summary|   passenger_count|    trip_distance|\n",
      "+-------+------------------+-----------------+\n",
      "|  count|           3542392|          3675412|\n",
      "|   mean|1.3846934500755421|6.206976298167358|\n",
      "| stddev|0.9302303297407405|640.8236808320215|\n",
      "|    min|               0.0|              0.0|\n",
      "|    max|               9.0|        389678.46|\n",
      "+-------+------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "yellowTaxiAnalyzedDF = (\n",
    "    # Calcula estadísticas descriptivas para  las columnas\n",
    "    # passenger_count y rip_distance\n",
    "                yellowTaxiDF.describe\n",
    "                            (\n",
    "                                \"passenger_count\",\n",
    "                                \"trip_distance\"\n",
    "                            )\n",
    "                       )\n",
    "\n",
    "yellowTaxiAnalyzedDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6abd45",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715bcbfa",
   "metadata": {},
   "source": [
    "#### 1. Accuracy Check: Filter inaccurate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d534187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "047667e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Returns a :class:`~pyspark.sql.Column` based on the given column name.\n",
      "\n",
      ".. versionadded:: 1.3.0\n",
      "\n",
      ".. versionchanged:: 3.4.0\n",
      "    Supports Spark Connect.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "col : str\n",
      "    the name for the column\n",
      "\n",
      "Returns\n",
      "-------\n",
      ":class:`~pyspark.sql.Column`\n",
      "    the corresponding column instance.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> col('x')\n",
      "Column<'x'>\n",
      ">>> column('x')\n",
      "Column<'x'>\n",
      "\u001b[0;31mFile:\u001b[0m      ~/Escritorio/curso_spark/env/lib/python3.8/site-packages/pyspark/sql/functions.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "col?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0c74a05b",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes de la operación = 3675412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Despues de la operación = 3422296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Display the count before operation\n",
    "print(\"Antes de la operación = \" + str( yellowTaxiDF.count()) )\n",
    "\n",
    "\n",
    "yellowTaxiDF_passenger_count_gt_0_and_trip_distance_gt_0 = (\n",
    "                  yellowTaxiDF\n",
    "# El método  where  se utiliza para filtrar las filas del DataFrame \n",
    "# donde el número de pasajeros es mayor que 0. \n",
    "                      .where(\"passenger_count > 0\")\n",
    "# , el método  filter  se utiliza para filtrar las filas del DataFrame \n",
    "# donde la distancia del viaje es mayor que 0.0. \n",
    "                      .filter(col(\"trip_distance\") > 0.0)\n",
    "               )\n",
    "\n",
    "\n",
    "# Display the count after operation\n",
    "print(\"Despues de la operación = \" + str( yellowTaxiDF_passenger_count_gt_0_and_trip_distance_gt_0.count()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c0a1783",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorId|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       1| 2022-09-30 21:03:41|  2022-09-30 21:18:39|            1.0|          1.7|       1.0|                 N|         249|         107|           1|        9.5|  3.0|    0.5|      2.65|         0.0|                  0.3|       15.95|                 2.5|        0.0|\n",
      "|       2| 2022-09-30 21:14:30|  2022-09-30 21:19:48|            2.0|         0.72|       1.0|                 N|         151|         238|           2|        5.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         9.3|                 2.5|        0.0|\n",
      "|       2| 2022-09-30 21:27:13|  2022-09-30 21:37:41|            1.0|         1.74|       1.0|                 N|         238|         166|           1|        9.0|  0.5|    0.5|      2.06|         0.0|                  0.3|       12.36|                 0.0|        0.0|\n",
      "|       1| 2022-09-30 21:22:52|  2022-09-30 21:52:14|            1.0|          6.8|       1.0|                 Y|         186|          41|           2|       25.5|  3.0|    0.5|       0.0|         0.0|                  0.3|        29.3|                 2.5|        0.0|\n",
      "|       2| 2022-09-30 21:33:19|  2022-09-30 21:44:51|            3.0|         1.88|       1.0|                 N|         162|         145|           2|       10.5|  0.5|    0.5|       0.0|         0.0|                  0.3|        14.3|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yellowTaxiDF_passenger_count_gt_0_and_trip_distance_gt_0.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4036dc5d",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorId: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yellowTaxiDF_passenger_count_gt_0_and_trip_distance_gt_0.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f78e49f",
   "metadata": {},
   "source": [
    "#### 2.a. Completeness Check: Drop rows with nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f0fe4c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(yellowTaxiDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1978ab5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DOLocationID',\n",
       " 'PULocationID',\n",
       " 'RatecodeID',\n",
       " 'VendorId',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_collect_as_arrow',\n",
       " '_ipython_key_completions_',\n",
       " '_jcols',\n",
       " '_jdf',\n",
       " '_jmap',\n",
       " '_joinAsOf',\n",
       " '_jseq',\n",
       " '_lazy_rdd',\n",
       " '_repr_html_',\n",
       " '_sc',\n",
       " '_schema',\n",
       " '_session',\n",
       " '_show_string',\n",
       " '_sort_cols',\n",
       " '_sql_ctx',\n",
       " '_support_repr_html',\n",
       " 'agg',\n",
       " 'airport_fee',\n",
       " 'alias',\n",
       " 'approxQuantile',\n",
       " 'cache',\n",
       " 'checkpoint',\n",
       " 'coalesce',\n",
       " 'colRegex',\n",
       " 'collect',\n",
       " 'columns',\n",
       " 'congestion_surcharge',\n",
       " 'corr',\n",
       " 'count',\n",
       " 'cov',\n",
       " 'createGlobalTempView',\n",
       " 'createOrReplaceGlobalTempView',\n",
       " 'createOrReplaceTempView',\n",
       " 'createTempView',\n",
       " 'crossJoin',\n",
       " 'crosstab',\n",
       " 'cube',\n",
       " 'describe',\n",
       " 'distinct',\n",
       " 'drop',\n",
       " 'dropDuplicates',\n",
       " 'dropDuplicatesWithinWatermark',\n",
       " 'drop_duplicates',\n",
       " 'dropna',\n",
       " 'dtypes',\n",
       " 'exceptAll',\n",
       " 'explain',\n",
       " 'extra',\n",
       " 'fare_amount',\n",
       " 'fillna',\n",
       " 'filter',\n",
       " 'first',\n",
       " 'foreach',\n",
       " 'foreachPartition',\n",
       " 'freqItems',\n",
       " 'groupBy',\n",
       " 'groupby',\n",
       " 'head',\n",
       " 'hint',\n",
       " 'improvement_surcharge',\n",
       " 'inputFiles',\n",
       " 'intersect',\n",
       " 'intersectAll',\n",
       " 'isEmpty',\n",
       " 'isLocal',\n",
       " 'isStreaming',\n",
       " 'is_cached',\n",
       " 'join',\n",
       " 'limit',\n",
       " 'localCheckpoint',\n",
       " 'mapInArrow',\n",
       " 'mapInPandas',\n",
       " 'melt',\n",
       " 'mta_tax',\n",
       " 'na',\n",
       " 'observe',\n",
       " 'offset',\n",
       " 'orderBy',\n",
       " 'pandas_api',\n",
       " 'passenger_count',\n",
       " 'payment_type',\n",
       " 'persist',\n",
       " 'printSchema',\n",
       " 'randomSplit',\n",
       " 'rdd',\n",
       " 'registerTempTable',\n",
       " 'repartition',\n",
       " 'repartitionByRange',\n",
       " 'replace',\n",
       " 'rollup',\n",
       " 'sameSemantics',\n",
       " 'sample',\n",
       " 'sampleBy',\n",
       " 'schema',\n",
       " 'select',\n",
       " 'selectExpr',\n",
       " 'semanticHash',\n",
       " 'show',\n",
       " 'sort',\n",
       " 'sortWithinPartitions',\n",
       " 'sparkSession',\n",
       " 'sql_ctx',\n",
       " 'stat',\n",
       " 'storageLevel',\n",
       " 'store_and_fwd_flag',\n",
       " 'subtract',\n",
       " 'summary',\n",
       " 'tail',\n",
       " 'take',\n",
       " 'tip_amount',\n",
       " 'to',\n",
       " 'toDF',\n",
       " 'toJSON',\n",
       " 'toLocalIterator',\n",
       " 'toPandas',\n",
       " 'to_koalas',\n",
       " 'to_pandas_on_spark',\n",
       " 'tolls_amount',\n",
       " 'total_amount',\n",
       " 'tpep_dropoff_datetime',\n",
       " 'tpep_pickup_datetime',\n",
       " 'transform',\n",
       " 'trip_distance',\n",
       " 'union',\n",
       " 'unionAll',\n",
       " 'unionByName',\n",
       " 'unpersist',\n",
       " 'unpivot',\n",
       " 'where',\n",
       " 'withColumn',\n",
       " 'withColumnRenamed',\n",
       " 'withColumns',\n",
       " 'withColumnsRenamed',\n",
       " 'withMetadata',\n",
       " 'withWatermark',\n",
       " 'write',\n",
       " 'writeStream',\n",
       " 'writeTo']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(yellowTaxiDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "78debe1e",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0myellowTaxiDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mhow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'any'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mthresh\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msubset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Returns a new :class:`DataFrame` omitting rows with null values.\n",
      ":func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n",
      "\n",
      ".. versionadded:: 1.3.1\n",
      "\n",
      ".. versionchanged:: 3.4.0\n",
      "    Supports Spark Connect.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "how : str, optional\n",
      "    'any' or 'all'.\n",
      "    If 'any', drop a row if it contains any nulls.\n",
      "    If 'all', drop a row only if all its values are null.\n",
      "thresh: int, optional\n",
      "    default None\n",
      "    If specified, drop rows that have less than `thresh` non-null values.\n",
      "    This overwrites the `how` parameter.\n",
      "subset : str, tuple or list, optional\n",
      "    optional list of column names to consider.\n",
      "\n",
      "Returns\n",
      "-------\n",
      ":class:`DataFrame`\n",
      "    DataFrame with null only rows excluded.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from pyspark.sql import Row\n",
      ">>> df = spark.createDataFrame([\n",
      "...     Row(age=10, height=80, name=\"Alice\"),\n",
      "...     Row(age=5, height=None, name=\"Bob\"),\n",
      "...     Row(age=None, height=None, name=\"Tom\"),\n",
      "...     Row(age=None, height=None, name=None),\n",
      "... ])\n",
      ">>> df.na.drop().show()\n",
      "+---+------+-----+\n",
      "|age|height| name|\n",
      "+---+------+-----+\n",
      "| 10|    80|Alice|\n",
      "+---+------+-----+\n",
      "\u001b[0;31mFile:\u001b[0m      ~/Escritorio/curso_spark/env/lib/python3.8/site-packages/pyspark/sql/dataframe.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "yellowTaxiDF.na.drop?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8ca2c2",
   "metadata": {},
   "source": [
    "La función  `na.drop()`  en Spark se utiliza para eliminar filas que contienen valores nulos en un DataFrame. A continuación se detallan los parámetros más comunes que se pueden utilizar con  `na.drop()` :\n",
    "\n",
    "1.  `how` : Especifica cómo se deben considerar los valores nulos para eliminar las filas. Los valores posibles son:\n",
    "   - `any` (predeterminado): Elimina la fila si contiene al menos un valor nulo en alguna de sus columnas.\n",
    "   - `all`: Elimina la fila solo si todos los valores son nulos.\n",
    "\n",
    "2.  `subset` : Lista de nombres de columnas para considerar al eliminar filas con valores nulos. Si se proporciona, solo se considerarán las columnas especificadas.\n",
    "\n",
    "3.  `thresh` : Número mínimo de valores no nulos que deben estar presentes en una fila para que no sea eliminada. Por ejemplo,  `thresh=2`  eliminará las filas que tengan menos de 2 valores no nulos.\n",
    "\n",
    "Estos son algunos de los parámetros más comunes que se pueden utilizar con la función  `na.drop()`  en Spark para personalizar el proceso de eliminación de filas con valores nulos en un DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "71bb90f7",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes de la operación = 3675412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Despues de la operaciónn = 3675412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Display the count before operation\n",
    "print(\"Antes de la operación = \" + str( yellowTaxiDF.count()) )\n",
    "\n",
    "\n",
    "yellowTaxiDF = (\n",
    "                   yellowTaxiDF    \n",
    "                          .na.drop(how='all')\n",
    "               )\n",
    "\n",
    "\n",
    "# Display the count after operation\n",
    "print(\"Despues de la operaciónn = \" + str( yellowTaxiDF.count()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23677d52",
   "metadata": {},
   "source": [
    "#### 2.b. Completeness Check: Replace nulls with default values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33e7b7c",
   "metadata": {},
   "source": [
    "La función  `na.fill()`  en Spark se utiliza para rellenar los valores nulos en un DataFrame con un valor específico. A continuación se detallan los parámetros más comunes que se pueden utilizar con  `na.fill()` :\n",
    "\n",
    "1.  `value` : \n",
    "\n",
    "    Valor que se utilizará para rellenar los valores nulos en el DataFrame. Puede ser un valor constante o un diccionario que mapea columnas a valores específicos.\n",
    "\n",
    "2.  `subset` : \n",
    "\n",
    "    Lista de nombres de columnas para considerar al rellenar los valores nulos. Si se proporciona, solo se considerarán las columnas especificadas.\n",
    "\n",
    "Por ejemplo, si deseas rellenar los valores nulos en todas las columnas de un DataFrame con el valor 0, puedes usar la función  `na.fill()`  de la siguiente manera:\n",
    "\n",
    "```python\n",
    "filled_df = original_df.na.fill(0)\n",
    "```\n",
    "\n",
    "Esto rellenará todos los valores nulos en el DataFrame  `original_df`  con el valor 0. También puedes especificar un diccionario de columnas y valores para rellenar valores nulos de manera más específica:\n",
    "\n",
    "```python\n",
    "filled_df = original_df.na.fill({'column1': 0, 'column2': 'No data'})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a4745686",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "defaultValueMap = {'payment_type': 5, 'RateCodeID': 1}\n",
    "\n",
    "\n",
    "yellowTaxiDF = (\n",
    "                   yellowTaxiDF    \n",
    "                      .na.fill(defaultValueMap)\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa475bfb",
   "metadata": {},
   "source": [
    "#### 3. Uniqueness Check: Drop duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7a3c8856",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes de la operación = 3675412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 19:11:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:11:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:11:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:11:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:11:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:11:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:11:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:11:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 49:====================================================> (193 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Despues de la operación = 3675410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Display the count before operation\n",
    "print(\"Antes de la operación = \" + str( yellowTaxiDF.count()) )\n",
    "\n",
    "\n",
    "yellowTaxiDF = (\n",
    "                   yellowTaxiDF\n",
    "                          .dropDuplicates()\n",
    "               )\n",
    "\n",
    "\n",
    "# Display the count after operation\n",
    "print(\"Despues de la operación = \" + str( yellowTaxiDF.count()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43297c69",
   "metadata": {},
   "source": [
    "#### 4. Timeliness Check: Remove records outside the bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "28aebb3f",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 19:15:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:15:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:15:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:15:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:15:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:15:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:15:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:15:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before operation = 3675410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 19:15:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:15:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:15:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:15:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:15:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:15:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:15:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:15:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 55:==================================================>   (188 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After operation = 3661886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Display the count before operation\n",
    "print(\"Before operation = \" + str( yellowTaxiDF.count()) )\n",
    "\n",
    "\n",
    "yellowTaxiDF = (\n",
    "    \n",
    "     yellowTaxiDF\n",
    "        .where(\"tpep_pickup_datetime >= '2022-10-01' AND tpep_dropoff_datetime < '2022-11-01'\")\n",
    ")\n",
    "\n",
    "\n",
    "# Display the count after operation\n",
    "print(\"After operation = \" + str( yellowTaxiDF.count()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dfdb46",
   "metadata": {},
   "source": [
    "### Chain all cleanup operation together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4b3eee3d",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 19:16:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:16:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:16:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:16:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:16:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:16:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:16:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:16:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 58:=================================================>    (182 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After operation = 3409776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "defaultValueMap = {'payment_type': 5, 'RateCodeID': 1}\n",
    "\n",
    "# Read file\n",
    "yellowTaxiDF = (\n",
    "                  spark\n",
    "                    .read\n",
    "                    .option(\"header\", \"true\")    \n",
    "                    .schema(yellowTaxiSchema)    \n",
    "                    .csv(\"datos/YellowTaxis_202210.csv\")\n",
    "               )\n",
    "\n",
    "# Cleanup data by applying data quality checks\n",
    "yellowTaxiDF = (\n",
    "                  yellowTaxiDF    \n",
    "    \n",
    "                      .where(\"passenger_count > 0\")\n",
    "    \n",
    "                      .filter(col(\"trip_distance\") > 0.0)\n",
    "                          \n",
    "                      .na.drop('all')\n",
    "    \n",
    "                      .na.fill(defaultValueMap)\n",
    "    \n",
    "                      .dropDuplicates()\n",
    "    \n",
    "                      .where(\"tpep_pickup_datetime >= '2022-10-01' AND tpep_dropoff_datetime < '2022-11-01'\")\n",
    "               )\n",
    "\n",
    "# Display the count after operation\n",
    "print(\"After operation = \" + str( yellowTaxiDF.count()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b2ecb41",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorId: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: double (nullable = false)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = false)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yellowTaxiDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082dcae4",
   "metadata": {},
   "source": [
    "## Transform Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394aee8a",
   "metadata": {},
   "source": [
    "#### 1. Select limited columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ff2e1cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import  col, column, cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "96fee4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Returns a :class:`~pyspark.sql.Column` based on the given column name.\n",
      "\n",
      ".. versionadded:: 1.3.0\n",
      "\n",
      ".. versionchanged:: 3.4.0\n",
      "    Supports Spark Connect.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "col : str\n",
      "    the name for the column\n",
      "\n",
      "Returns\n",
      "-------\n",
      ":class:`~pyspark.sql.Column`\n",
      "    the corresponding column instance.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> col('x')\n",
      "Column<'x'>\n",
      ">>> column('x')\n",
      "Column<'x'>\n",
      "\u001b[0;31mFile:\u001b[0m      ~/Escritorio/curso_spark/env/lib/python3.8/site-packages/pyspark/sql/functions.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "973b682d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Cast a value to a type.\n",
      "\n",
      "This returns the value unchanged.  To the type checker this\n",
      "signals that the return value has the designated type, but at\n",
      "runtime we intentionally don't check anything (we want this\n",
      "to be as fast as possible).\n",
      "\u001b[0;31mFile:\u001b[0m      /usr/lib/python3.8/typing.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "cast?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4b155850",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- TripDistance: double (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PUlocationID: integer (nullable = true)\n",
      " |-- DOlocationID: integer (nullable = true)\n",
      " |-- RatecodeID: double (nullable = false)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yellowTaxiDF_2 = (\n",
    "                   yellowTaxiDF\n",
    "\n",
    "                        # Select only limited columns\n",
    "                        .select(\n",
    "                                  \"VendorID\",\n",
    "                             \n",
    "                                  col(\"passenger_count\").cast(IntegerType()),\n",
    "                            \n",
    "                                  column(\"trip_distance\").alias(\"TripDistance\"),\n",
    "                            \n",
    "                                  \"tpep_pickup_datetime\",\n",
    "                            \n",
    "                                  \"tpep_dropoff_datetime\",\n",
    "                                  \"PUlocationID\",\n",
    "                                  \"DOlocationID\",\n",
    "                                  \"RatecodeID\",\n",
    "                                  \"total_amount\",\n",
    "                                  \"payment_type\"\n",
    "                               )\n",
    "    \n",
    "                        # Don't run, since airport_fee has not been selected above    \n",
    "                        # .drop(\"airport_fee\") \n",
    "               )\n",
    "\n",
    "yellowTaxiDF_2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bbdd7173",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 19:21:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:21:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:21:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:21:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:21:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:21:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:21:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:21:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 60:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+------------+--------------------+---------------------+------------+------------+----------+------------+------------+\n",
      "|VendorID|passenger_count|TripDistance|tpep_pickup_datetime|tpep_dropoff_datetime|PUlocationID|DOlocationID|RatecodeID|total_amount|payment_type|\n",
      "+--------+---------------+------------+--------------------+---------------------+------------+------------+----------+------------+------------+\n",
      "|       2|              2|         1.8| 2022-10-01 00:18:28|  2022-10-01 00:26:54|          48|          68|       1.0|       14.76|           1|\n",
      "|       2|              2|        1.41| 2022-10-01 00:40:53|  2022-10-01 00:47:01|         211|         261|       1.0|       12.36|           1|\n",
      "|       2|              1|        1.54| 2022-10-01 00:07:52|  2022-10-01 00:15:34|          79|         170|       1.0|       13.56|           1|\n",
      "|       2|              1|       12.38| 2022-10-01 00:35:27|  2022-10-01 01:03:59|         231|         159|       1.0|       48.36|           1|\n",
      "|       2|              1|        1.14| 2022-10-01 01:00:26|  2022-10-01 01:05:14|         231|         158|       1.0|       12.74|           1|\n",
      "+--------+---------------+------------+--------------------+---------------------+------------+------------+----------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "yellowTaxiDF_2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1ed87a8b",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- TripDistance: double (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PUlocationID: integer (nullable = true)\n",
      " |-- DOlocationID: integer (nullable = true)\n",
      " |-- RatecodeID: double (nullable = false)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yellowTaxiDF = (\n",
    "                   yellowTaxiDF\n",
    "\n",
    "                        # Select only limited columns\n",
    "                        .select(\n",
    "                                  \"VendorID\",\n",
    "                             \n",
    "                                  col(\"passenger_count\").cast(IntegerType()),\n",
    "                            \n",
    "                                  column(\"trip_distance\").alias(\"TripDistance\"),\n",
    "                            \n",
    "                                  \"tpep_pickup_datetime\",\n",
    "                            \n",
    "                                  \"tpep_dropoff_datetime\",\n",
    "                                  \"PUlocationID\",\n",
    "                                  \"DOlocationID\",\n",
    "                                  \"RatecodeID\",\n",
    "                                  \"total_amount\",\n",
    "                                  \"payment_type\"\n",
    "                               )\n",
    "    \n",
    "                        # Don't run, since airport_fee has not been selected above    \n",
    "                        # .drop(\"airport_fee\") \n",
    "               )\n",
    "\n",
    "yellowTaxiDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c621bd3e",
   "metadata": {},
   "source": [
    "### 2. Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ed0a0beb",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- PassengerCount: integer (nullable = true)\n",
      " |-- TripDistance: double (nullable = true)\n",
      " |-- PickupTime: timestamp (nullable = true)\n",
      " |-- DropTime: timestamp (nullable = true)\n",
      " |-- PickupLocationId: integer (nullable = true)\n",
      " |-- DropLocationId: integer (nullable = true)\n",
      " |-- RatecodeID: double (nullable = false)\n",
      " |-- TotalAmount: double (nullable = true)\n",
      " |-- PaymentType: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yellowTaxiDF = (\n",
    "                   yellowTaxiDF                        \n",
    "                        \n",
    "                        .withColumnRenamed(\"passenger_count\", \"PassengerCount\")\n",
    "    \n",
    "                        .withColumnRenamed(\"tpep_pickup_datetime\", \"PickupTime\")\n",
    "                        .withColumnRenamed(\"tpep_dropoff_datetime\", \"DropTime\")\n",
    "                        .withColumnRenamed(\"PUlocationID\", \"PickupLocationId\")\n",
    "                        .withColumnRenamed(\"DOlocationID\", \"DropLocationId\")\n",
    "                        .withColumnRenamed(\"total_amount\", \"TotalAmount\")\n",
    "                        .withColumnRenamed(\"payment_type\", \"PaymentType\")    \n",
    "               )\n",
    "\n",
    "yellowTaxiDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "31c45270",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 19:23:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:23:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:23:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:23:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:23:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:23:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:23:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:23:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 62:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+------------+-------------------+-------------------+----------------+--------------+----------+-----------+-----------+\n",
      "|VendorID|PassengerCount|TripDistance|         PickupTime|           DropTime|PickupLocationId|DropLocationId|RatecodeID|TotalAmount|PaymentType|\n",
      "+--------+--------------+------------+-------------------+-------------------+----------------+--------------+----------+-----------+-----------+\n",
      "|       2|             2|         1.8|2022-10-01 00:18:28|2022-10-01 00:26:54|              48|            68|       1.0|      14.76|          1|\n",
      "|       2|             2|        1.41|2022-10-01 00:40:53|2022-10-01 00:47:01|             211|           261|       1.0|      12.36|          1|\n",
      "+--------+--------------+------------+-------------------+-------------------+----------------+--------------+----------+-----------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "yellowTaxiDF.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0fe8c2",
   "metadata": {},
   "source": [
    "### 3.a. Create derived columns - TripYear, TripMonth, TripDay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ab99b256",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import  year, month, dayofmonth, expr, length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c687e644",
   "metadata": {},
   "source": [
    "#### help function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "afd99d6b",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'ColumnOrName'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Computes the character length of string data or number of bytes of binary data.\n",
      "The length of character data includes the trailing spaces. The length of binary data\n",
      "includes binary zeros.\n",
      "\n",
      ".. versionadded:: 1.5.0\n",
      "\n",
      ".. versionchanged:: 3.4.0\n",
      "    Supports Spark Connect.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "col : :class:`~pyspark.sql.Column` or str\n",
      "    target column to work on.\n",
      "\n",
      "Returns\n",
      "-------\n",
      ":class:`~pyspark.sql.Column`\n",
      "    length of the value.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> spark.createDataFrame([('ABC ',)], ['a']).select(length('a').alias('length')).collect()\n",
      "[Row(length=4)]\n",
      "\u001b[0;31mFile:\u001b[0m      ~/Escritorio/curso_spark/env/lib/python3.8/site-packages/pyspark/sql/functions.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "length?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b014b43c",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|   a|\n",
      "+----+\n",
      "|ABC |\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame([('ABC ',)], ['a'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f24ac578",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|length|\n",
      "+------+\n",
      "|     4|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(length('a').alias('length')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Parses the expression string into the column that it represents\n",
      "\n",
      ".. versionadded:: 1.5.0\n",
      "\n",
      ".. versionchanged:: 3.4.0\n",
      "    Supports Spark Connect.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "str : str\n",
      "    expression defined in string.\n",
      "\n",
      "Returns\n",
      "-------\n",
      ":class:`~pyspark.sql.Column`\n",
      "    column representing the expression.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> df = spark.createDataFrame([[\"Alice\"], [\"Bob\"]], [\"name\"])\n",
      ">>> df.select(\"name\", expr(\"length(name)\")).show()\n",
      "+-----+------------+\n",
      "| name|length(name)|\n",
      "+-----+------------+\n",
      "|Alice|           5|\n",
      "|  Bob|           3|\n",
      "+-----+------------+\n",
      "\u001b[0;31mFile:\u001b[0m      ~/Escritorio/curso_spark/env/lib/python3.8/site-packages/pyspark/sql/functions.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "expr?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4f178c14",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mmonth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'ColumnOrName'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Extract the month of a given date/timestamp as integer.\n",
      "\n",
      ".. versionadded:: 1.5.0\n",
      "\n",
      ".. versionchanged:: 3.4.0\n",
      "    Supports Spark Connect.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "col : :class:`~pyspark.sql.Column` or str\n",
      "    target date/timestamp column to work on.\n",
      "\n",
      "Returns\n",
      "-------\n",
      ":class:`~pyspark.sql.Column`\n",
      "    month part of the date/timestamp as integer.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      ">>> df.select(month('dt').alias('month')).collect()\n",
      "[Row(month=4)]\n",
      "\u001b[0;31mFile:\u001b[0m      ~/Escritorio/curso_spark/env/lib/python3.8/site-packages/pyspark/sql/functions.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "715b9ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'ColumnOrName'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Extract the year of a given date/timestamp as integer.\n",
      "\n",
      ".. versionadded:: 1.5.0\n",
      "\n",
      ".. versionchanged:: 3.4.0\n",
      "    Supports Spark Connect.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "col : :class:`~pyspark.sql.Column` or str\n",
      "    target date/timestamp column to work on.\n",
      "\n",
      "Returns\n",
      "-------\n",
      ":class:`~pyspark.sql.Column`\n",
      "    year part of the date/timestamp as integer.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      ">>> df.select(year('dt').alias('year')).collect()\n",
      "[Row(year=2015)]\n",
      "\u001b[0;31mFile:\u001b[0m      ~/Escritorio/curso_spark/env/lib/python3.8/site-packages/pyspark/sql/functions.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f680549c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mdayofmonth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'ColumnOrName'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Extract the day of the month of a given date/timestamp as integer.\n",
      "\n",
      ".. versionadded:: 1.5.0\n",
      "\n",
      ".. versionchanged:: 3.4.0\n",
      "    Supports Spark Connect.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "col : :class:`~pyspark.sql.Column` or str\n",
      "    target date/timestamp column to work on.\n",
      "\n",
      "Returns\n",
      "-------\n",
      ":class:`~pyspark.sql.Column`\n",
      "    day of the month for given date/timestamp as integer.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      ">>> df.select(dayofmonth('dt').alias('day')).collect()\n",
      "[Row(day=8)]\n",
      "\u001b[0;31mFile:\u001b[0m      ~/Escritorio/curso_spark/env/lib/python3.8/site-packages/pyspark/sql/functions.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "dayofmonth?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e101f7d3",
   "metadata": {},
   "source": [
    "#### ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "546d178e",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- PassengerCount: integer (nullable = true)\n",
      " |-- TripDistance: double (nullable = true)\n",
      " |-- PickupTime: timestamp (nullable = true)\n",
      " |-- DropTime: timestamp (nullable = true)\n",
      " |-- PickupLocationId: integer (nullable = true)\n",
      " |-- DropLocationId: integer (nullable = true)\n",
      " |-- RatecodeID: double (nullable = false)\n",
      " |-- TotalAmount: double (nullable = true)\n",
      " |-- PaymentType: integer (nullable = false)\n",
      " |-- TripYear: integer (nullable = true)\n",
      " |-- TripMonth: integer (nullable = true)\n",
      " |-- TripDay: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create derived columns for year, month and day\n",
    "yellowTaxiDF = (\n",
    "                  yellowTaxiDF\n",
    "    \n",
    "                        .withColumn(\"TripYear\", year(col(\"PickupTime\")))\n",
    "    \n",
    "                        .select(\n",
    "                                    \"*\",\n",
    "                            \n",
    "                                    expr(\"month(PickupTime) AS TripMonth\"),\n",
    "                            \n",
    "                                    dayofmonth(col(\"PickupTime\")).alias(\"TripDay\")\n",
    "                               )\n",
    "               )\n",
    "\n",
    "yellowTaxiDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "39d059e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 19:31:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:31:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:31:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:31:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:31:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:31:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:31:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:31:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 64:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|TripYear|TripMonth|TripDay|\n",
      "+--------+---------+-------+\n",
      "|    2022|       10|      1|\n",
      "|    2022|       10|      1|\n",
      "|    2022|       10|      1|\n",
      "|    2022|       10|      1|\n",
      "|    2022|       10|      1|\n",
      "+--------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "yellowTaxiDF[\"TripYear\", \"TripMonth\", \"TripDay\"].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b96fac",
   "metadata": {},
   "source": [
    "#### 3.b. Create derived column - TripTimeInMinutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "845a5fdd",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "841bc2ac",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0munix_timestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtimestamp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ColumnOrName'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mformat\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'yyyy-MM-dd HH:mm:ss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Convert time string with given pattern ('yyyy-MM-dd HH:mm:ss', by default)\n",
      "to Unix time stamp (in seconds), using the default timezone and the default\n",
      "locale, returns null if failed.\n",
      "\n",
      "if `timestamp` is None, then it returns current timestamp.\n",
      "\n",
      ".. versionadded:: 1.5.0\n",
      "\n",
      ".. versionchanged:: 3.4.0\n",
      "    Supports Spark Connect.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "timestamp : :class:`~pyspark.sql.Column` or str, optional\n",
      "    timestamps of string values.\n",
      "format : str, optional\n",
      "    alternative format to use for converting (default: yyyy-MM-dd HH:mm:ss).\n",
      "\n",
      "Returns\n",
      "-------\n",
      ":class:`~pyspark.sql.Column`\n",
      "    unix time as long integer.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      ">>> time_df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      ">>> time_df.select(unix_timestamp('dt', 'yyyy-MM-dd').alias('unix_time')).collect()\n",
      "[Row(unix_time=1428476400)]\n",
      ">>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "\u001b[0;31mFile:\u001b[0m      ~/Escritorio/curso_spark/env/lib/python3.8/site-packages/pyspark/sql/functions.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "unix_timestamp?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2138ad6b",
   "metadata": {},
   "source": [
    "intanciamos la funcion round de pyspark con un alias para no pisar la funcion round nativa de python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "16094077",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round as spround "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8f0a662f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mspround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'ColumnOrName'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Round the given value to `scale` decimal places using HALF_UP rounding mode if `scale` >= 0\n",
      "or at integral part when `scale` < 0.\n",
      "\n",
      ".. versionadded:: 1.5.0\n",
      "\n",
      ".. versionchanged:: 3.4.0\n",
      "    Supports Spark Connect.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "col : :class:`~pyspark.sql.Column` or str\n",
      "    input column to round.\n",
      "scale : int optional default 0\n",
      "    scale value.\n",
      "\n",
      "Returns\n",
      "-------\n",
      ":class:`~pyspark.sql.Column`\n",
      "    rounded values.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> spark.createDataFrame([(2.5,)], ['a']).select(round('a', 0).alias('r')).collect()\n",
      "[Row(r=3.0)]\n",
      "\u001b[0;31mFile:\u001b[0m      ~/Escritorio/curso_spark/env/lib/python3.8/site-packages/pyspark/sql/functions.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "spround?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1c5a35ef",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- PassengerCount: integer (nullable = true)\n",
      " |-- TripDistance: double (nullable = true)\n",
      " |-- PickupTime: timestamp (nullable = true)\n",
      " |-- DropTime: timestamp (nullable = true)\n",
      " |-- PickupLocationId: integer (nullable = true)\n",
      " |-- DropLocationId: integer (nullable = true)\n",
      " |-- RatecodeID: double (nullable = false)\n",
      " |-- TotalAmount: double (nullable = true)\n",
      " |-- PaymentType: integer (nullable = false)\n",
      " |-- TripYear: integer (nullable = true)\n",
      " |-- TripMonth: integer (nullable = true)\n",
      " |-- TripDay: integer (nullable = true)\n",
      " |-- TripTimeInMinutes: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Option 1\n",
    "yellowTaxiDF = (\n",
    "\n",
    "yellowTaxiDF.withColumn(\"TripTimeInMinutes\", \n",
    "            spround(\n",
    "                  unix_timestamp(col(\"DropTime\")) - unix_timestamp(col(\"PickupTime\")))\n",
    "                  / 60\n",
    "                        )\n",
    "            )\n",
    "\n",
    "yellowTaxiDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "32fcdc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 19:45:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:45:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:45:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:45:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:45:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:45:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:45:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:45:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 69:=====================================================>(197 + 3) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|  TripTimeInMinutes|\n",
      "+-------+-------------------+\n",
      "|  count|            3409776|\n",
      "|   mean|  18.07607436187402|\n",
      "| stddev| 45.010463831295404|\n",
      "|    min|-1397.5333333333333|\n",
      "|    max|            5949.95|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "yellowTaxiDF.describe(\"TripTimeInMinutes\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56ce2fe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "429d3c63",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- PassengerCount: integer (nullable = true)\n",
      " |-- TripDistance: double (nullable = true)\n",
      " |-- PickupTime: timestamp (nullable = true)\n",
      " |-- DropTime: timestamp (nullable = true)\n",
      " |-- PickupLocationId: integer (nullable = true)\n",
      " |-- DropLocationId: integer (nullable = true)\n",
      " |-- RatecodeID: double (nullable = false)\n",
      " |-- TotalAmount: double (nullable = true)\n",
      " |-- PaymentType: integer (nullable = false)\n",
      " |-- TripYear: integer (nullable = true)\n",
      " |-- TripMonth: integer (nullable = true)\n",
      " |-- TripDay: integer (nullable = true)\n",
      " |-- TripTimeInMinutes: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Option 2\n",
    "\n",
    "tripTimeInSecondsExpr = unix_timestamp(col(\"DropTime\")) - unix_timestamp(col(\"PickupTime\"))\n",
    "\n",
    "\n",
    "tripTimeInMinutesExpr = spround(tripTimeInSecondsExpr / 60)\n",
    "\n",
    "\n",
    "yellowTaxiDF = (\n",
    "                  yellowTaxiDF\n",
    "                        .withColumn(\"TripTimeInMinutes\", tripTimeInMinutesExpr)\n",
    "               )\n",
    "\n",
    "yellowTaxiDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e237ad9",
   "metadata": {},
   "source": [
    "#### 3.c. Create derived column - TripType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "83f2bbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e806bd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      "If :func:`pyspark.sql.Column.otherwise` is not invoked, None is returned for unmatched\n",
      "conditions.\n",
      "\n",
      ".. versionadded:: 1.4.0\n",
      "\n",
      ".. versionchanged:: 3.4.0\n",
      "    Supports Spark Connect.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "condition : :class:`~pyspark.sql.Column`\n",
      "    a boolean :class:`~pyspark.sql.Column` expression.\n",
      "value :\n",
      "    a literal value, or a :class:`~pyspark.sql.Column` expression.\n",
      "\n",
      "Returns\n",
      "-------\n",
      ":class:`~pyspark.sql.Column`\n",
      "    column representing when expression.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> df = spark.range(3)\n",
      ">>> df.select(when(df['id'] == 2, 3).otherwise(4).alias(\"age\")).show()\n",
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "|  4|\n",
      "|  4|\n",
      "|  3|\n",
      "+---+\n",
      "\n",
      ">>> df.select(when(df.id == 2, df.id + 1).alias(\"age\")).show()\n",
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|NULL|\n",
      "|NULL|\n",
      "|   3|\n",
      "+----+\n",
      "\u001b[0;31mFile:\u001b[0m      ~/Escritorio/curso_spark/env/lib/python3.8/site-packages/pyspark/sql/functions.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "when?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "590c6bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- PassengerCount: integer (nullable = true)\n",
      " |-- TripDistance: double (nullable = true)\n",
      " |-- PickupTime: timestamp (nullable = true)\n",
      " |-- DropTime: timestamp (nullable = true)\n",
      " |-- PickupLocationId: integer (nullable = true)\n",
      " |-- DropLocationId: integer (nullable = true)\n",
      " |-- TotalAmount: double (nullable = true)\n",
      " |-- PaymentType: integer (nullable = false)\n",
      " |-- TripYear: integer (nullable = true)\n",
      " |-- TripMonth: integer (nullable = true)\n",
      " |-- TripDay: integer (nullable = true)\n",
      " |-- TripTimeInMinutes: double (nullable = true)\n",
      " |-- TripType: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tripTypeColumn = (\n",
    "                    when(\n",
    "                            col(\"RatecodeID\") == 6,\n",
    "                              \"SharedTrip\"\n",
    "                         )\n",
    "                    .otherwise(\"SoloTrip\")\n",
    "                 )    \n",
    "\n",
    "\n",
    "yellowTaxiDF = (\n",
    "                  yellowTaxiDF\n",
    "    \n",
    "                        .withColumn(\"TripType\", tripTypeColumn)\n",
    "    \n",
    "                        .drop(\"RatecodeID\")\n",
    "               )\n",
    "\n",
    "yellowTaxiDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cf9248",
   "metadata": {},
   "source": [
    "### Check Execution Plans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0283d908",
   "metadata": {},
   "source": [
    "Este método se utiliza para mostrar el plan de ejecución detallado de una consulta en Spark.\n",
    "\n",
    "El parámetro mode se establece en \"extended\", lo que significa que se proporcionará una explicación extendida del plan de ejecución. Esto incluirá información adicional, como las métricas de tiempo de ejecución estimadas y la estructura física de los datos.\n",
    "\n",
    "La salida del método explain() no es un DataFrame regular, sino más bien una visualización textual del plan de ejecución. Proporciona una descripción paso a paso de cada etapa de la ejecución, desde la lectura de los datos hasta las operaciones finales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bbeb87ee",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [VendorID#1494, PassengerCount#1832, TripDistance#1820, PickupTime#1843, DropTime#1854, PickupLocationId#1865, DropLocationId#1876, RatecodeID#1571, TotalAmount#1887, PaymentType#1898, TripYear#1931, TripMonth#1943, TripDay#1944, round(((unix_timestamp('DropTime, yyyy-MM-dd HH:mm:ss, None, false) - unix_timestamp('PickupTime, yyyy-MM-dd HH:mm:ss, None, false)) / 60), 0) AS TripTimeInMinutes#1973]\n",
      "+- Project [VendorID#1494, PassengerCount#1832, TripDistance#1820, PickupTime#1843, DropTime#1854, PickupLocationId#1865, DropLocationId#1876, RatecodeID#1571, TotalAmount#1887, PaymentType#1898, TripYear#1931, TripMonth#1943, TripDay#1944, (cast(round((unix_timestamp(DropTime#1854, yyyy-MM-dd HH:mm:ss, Some(America/Argentina/Buenos_Aires), false) - unix_timestamp(PickupTime#1843, yyyy-MM-dd HH:mm:ss, Some(America/Argentina/Buenos_Aires), false)), 0) as double) / cast(60 as double)) AS TripTimeInMinutes#1958]\n",
      "   +- Project [VendorID#1494, PassengerCount#1832, TripDistance#1820, PickupTime#1843, DropTime#1854, PickupLocationId#1865, DropLocationId#1876, RatecodeID#1571, TotalAmount#1887, PaymentType#1898, TripYear#1931, month(cast(PickupTime#1843 as date)) AS TripMonth#1943, dayofmonth(cast(PickupTime#1843 as date)) AS TripDay#1944]\n",
      "      +- Project [VendorID#1494, PassengerCount#1832, TripDistance#1820, PickupTime#1843, DropTime#1854, PickupLocationId#1865, DropLocationId#1876, RatecodeID#1571, TotalAmount#1887, PaymentType#1898, year(cast(PickupTime#1843 as date)) AS TripYear#1931]\n",
      "         +- Project [VendorID#1494, PassengerCount#1832, TripDistance#1820, PickupTime#1843, DropTime#1854, PickupLocationId#1865, DropLocationId#1876, RatecodeID#1571, TotalAmount#1887, payment_type#1572 AS PaymentType#1898]\n",
      "            +- Project [VendorID#1494, PassengerCount#1832, TripDistance#1820, PickupTime#1843, DropTime#1854, PickupLocationId#1865, DropLocationId#1876, RatecodeID#1571, total_amount#1510 AS TotalAmount#1887, payment_type#1572]\n",
      "               +- Project [VendorID#1494, PassengerCount#1832, TripDistance#1820, PickupTime#1843, DropTime#1854, PickupLocationId#1865, DOlocationID#1502 AS DropLocationId#1876, RatecodeID#1571, total_amount#1510, payment_type#1572]\n",
      "                  +- Project [VendorID#1494, PassengerCount#1832, TripDistance#1820, PickupTime#1843, DropTime#1854, PUlocationID#1501 AS PickupLocationId#1865, DOlocationID#1502, RatecodeID#1571, total_amount#1510, payment_type#1572]\n",
      "                     +- Project [VendorID#1494, PassengerCount#1832, TripDistance#1820, PickupTime#1843, tpep_dropoff_datetime#1496 AS DropTime#1854, PUlocationID#1501, DOlocationID#1502, RatecodeID#1571, total_amount#1510, payment_type#1572]\n",
      "                        +- Project [VendorID#1494, PassengerCount#1832, TripDistance#1820, tpep_pickup_datetime#1495 AS PickupTime#1843, tpep_dropoff_datetime#1496, PUlocationID#1501, DOlocationID#1502, RatecodeID#1571, total_amount#1510, payment_type#1572]\n",
      "                           +- Project [VendorID#1494, passenger_count#1821 AS PassengerCount#1832, TripDistance#1820, tpep_pickup_datetime#1495, tpep_dropoff_datetime#1496, PUlocationID#1501, DOlocationID#1502, RatecodeID#1571, total_amount#1510, payment_type#1572]\n",
      "                              +- Project [VendorID#1494, cast(passenger_count#1497 as int) AS passenger_count#1821, trip_distance#1498 AS TripDistance#1820, tpep_pickup_datetime#1495, tpep_dropoff_datetime#1496, PUlocationID#1501, DOlocationID#1502, RatecodeID#1571, total_amount#1510, payment_type#1572]\n",
      "                                 +- Filter ((tpep_pickup_datetime#1495 >= cast(2022-10-01 as timestamp)) AND (tpep_dropoff_datetime#1496 < cast(2022-11-01 as timestamp)))\n",
      "                                    +- Deduplicate [DOLocationID#1502, improvement_surcharge#1509, tpep_dropoff_datetime#1496, PULocationID#1501, trip_distance#1498, tolls_amount#1508, RatecodeID#1571, VendorId#1494, tip_amount#1507, payment_type#1572, fare_amount#1504, passenger_count#1497, store_and_fwd_flag#1500, extra#1505, airport_fee#1512, congestion_surcharge#1511, total_amount#1510, tpep_pickup_datetime#1495, mta_tax#1506]\n",
      "                                       +- Project [VendorId#1494, tpep_pickup_datetime#1495, tpep_dropoff_datetime#1496, passenger_count#1497, trip_distance#1498, coalesce(nanvl(RatecodeID#1499, cast(null as double)), cast(1 as double)) AS RatecodeID#1571, store_and_fwd_flag#1500, PULocationID#1501, DOLocationID#1502, coalesce(payment_type#1503, cast(5 as int)) AS payment_type#1572, fare_amount#1504, extra#1505, mta_tax#1506, tip_amount#1507, tolls_amount#1508, improvement_surcharge#1509, total_amount#1510, congestion_surcharge#1511, airport_fee#1512]\n",
      "                                          +- Filter atleastnnonnulls(1, VendorId#1494, tpep_pickup_datetime#1495, tpep_dropoff_datetime#1496, passenger_count#1497, trip_distance#1498, RatecodeID#1499, store_and_fwd_flag#1500, PULocationID#1501, DOLocationID#1502, payment_type#1503, fare_amount#1504, extra#1505, mta_tax#1506, tip_amount#1507, tolls_amount#1508, improvement_surcharge#1509, total_amount#1510, congestion_surcharge#1511, airport_fee#1512)\n",
      "                                             +- Filter (trip_distance#1498 > 0.0)\n",
      "                                                +- Filter (passenger_count#1497 > cast(0 as double))\n",
      "                                                   +- Relation [VendorId#1494,tpep_pickup_datetime#1495,tpep_dropoff_datetime#1496,passenger_count#1497,trip_distance#1498,RatecodeID#1499,store_and_fwd_flag#1500,PULocationID#1501,DOLocationID#1502,payment_type#1503,fare_amount#1504,extra#1505,mta_tax#1506,tip_amount#1507,tolls_amount#1508,improvement_surcharge#1509,total_amount#1510,congestion_surcharge#1511,airport_fee#1512] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "VendorID: int, PassengerCount: int, TripDistance: double, PickupTime: timestamp, DropTime: timestamp, PickupLocationId: int, DropLocationId: int, RatecodeID: double, TotalAmount: double, PaymentType: int, TripYear: int, TripMonth: int, TripDay: int, TripTimeInMinutes: double\n",
      "Project [VendorID#1494, PassengerCount#1832, TripDistance#1820, PickupTime#1843, DropTime#1854, PickupLocationId#1865, DropLocationId#1876, RatecodeID#1571, TotalAmount#1887, PaymentType#1898, TripYear#1931, TripMonth#1943, TripDay#1944, round((cast((unix_timestamp(DropTime#1854, yyyy-MM-dd HH:mm:ss, Some(America/Argentina/Buenos_Aires), false) - unix_timestamp(PickupTime#1843, yyyy-MM-dd HH:mm:ss, Some(America/Argentina/Buenos_Aires), false)) as double) / cast(60 as double)), 0) AS TripTimeInMinutes#1973]\n",
      "+- Project [VendorID#1494, PassengerCount#1832, TripDistance#1820, PickupTime#1843, DropTime#1854, PickupLocationId#1865, DropLocationId#1876, RatecodeID#1571, TotalAmount#1887, PaymentType#1898, TripYear#1931, TripMonth#1943, TripDay#1944, (cast(round((unix_timestamp(DropTime#1854, yyyy-MM-dd HH:mm:ss, Some(America/Argentina/Buenos_Aires), false) - unix_timestamp(PickupTime#1843, yyyy-MM-dd HH:mm:ss, Some(America/Argentina/Buenos_Aires), false)), 0) as double) / cast(60 as double)) AS TripTimeInMinutes#1958]\n",
      "   +- Project [VendorID#1494, PassengerCount#1832, TripDistance#1820, PickupTime#1843, DropTime#1854, PickupLocationId#1865, DropLocationId#1876, RatecodeID#1571, TotalAmount#1887, PaymentType#1898, TripYear#1931, month(cast(PickupTime#1843 as date)) AS TripMonth#1943, dayofmonth(cast(PickupTime#1843 as date)) AS TripDay#1944]\n",
      "      +- Project [VendorID#1494, PassengerCount#1832, TripDistance#1820, PickupTime#1843, DropTime#1854, PickupLocationId#1865, DropLocationId#1876, RatecodeID#1571, TotalAmount#1887, PaymentType#1898, year(cast(PickupTime#1843 as date)) AS TripYear#1931]\n",
      "         +- Project [VendorID#1494, PassengerCount#1832, TripDistance#1820, PickupTime#1843, DropTime#1854, PickupLocationId#1865, DropLocationId#1876, RatecodeID#1571, TotalAmount#1887, payment_type#1572 AS PaymentType#1898]\n",
      "            +- Project [VendorID#1494, PassengerCount#1832, TripDistance#1820, PickupTime#1843, DropTime#1854, PickupLocationId#1865, DropLocationId#1876, RatecodeID#1571, total_amount#1510 AS TotalAmount#1887, payment_type#1572]\n",
      "               +- Project [VendorID#1494, PassengerCount#1832, TripDistance#1820, PickupTime#1843, DropTime#1854, PickupLocationId#1865, DOlocationID#1502 AS DropLocationId#1876, RatecodeID#1571, total_amount#1510, payment_type#1572]\n",
      "                  +- Project [VendorID#1494, PassengerCount#1832, TripDistance#1820, PickupTime#1843, DropTime#1854, PUlocationID#1501 AS PickupLocationId#1865, DOlocationID#1502, RatecodeID#1571, total_amount#1510, payment_type#1572]\n",
      "                     +- Project [VendorID#1494, PassengerCount#1832, TripDistance#1820, PickupTime#1843, tpep_dropoff_datetime#1496 AS DropTime#1854, PUlocationID#1501, DOlocationID#1502, RatecodeID#1571, total_amount#1510, payment_type#1572]\n",
      "                        +- Project [VendorID#1494, PassengerCount#1832, TripDistance#1820, tpep_pickup_datetime#1495 AS PickupTime#1843, tpep_dropoff_datetime#1496, PUlocationID#1501, DOlocationID#1502, RatecodeID#1571, total_amount#1510, payment_type#1572]\n",
      "                           +- Project [VendorID#1494, passenger_count#1821 AS PassengerCount#1832, TripDistance#1820, tpep_pickup_datetime#1495, tpep_dropoff_datetime#1496, PUlocationID#1501, DOlocationID#1502, RatecodeID#1571, total_amount#1510, payment_type#1572]\n",
      "                              +- Project [VendorID#1494, cast(passenger_count#1497 as int) AS passenger_count#1821, trip_distance#1498 AS TripDistance#1820, tpep_pickup_datetime#1495, tpep_dropoff_datetime#1496, PUlocationID#1501, DOlocationID#1502, RatecodeID#1571, total_amount#1510, payment_type#1572]\n",
      "                                 +- Filter ((tpep_pickup_datetime#1495 >= cast(2022-10-01 as timestamp)) AND (tpep_dropoff_datetime#1496 < cast(2022-11-01 as timestamp)))\n",
      "                                    +- Deduplicate [DOLocationID#1502, improvement_surcharge#1509, tpep_dropoff_datetime#1496, PULocationID#1501, trip_distance#1498, tolls_amount#1508, RatecodeID#1571, VendorId#1494, tip_amount#1507, payment_type#1572, fare_amount#1504, passenger_count#1497, store_and_fwd_flag#1500, extra#1505, airport_fee#1512, congestion_surcharge#1511, total_amount#1510, tpep_pickup_datetime#1495, mta_tax#1506]\n",
      "                                       +- Project [VendorId#1494, tpep_pickup_datetime#1495, tpep_dropoff_datetime#1496, passenger_count#1497, trip_distance#1498, coalesce(nanvl(RatecodeID#1499, cast(null as double)), cast(1 as double)) AS RatecodeID#1571, store_and_fwd_flag#1500, PULocationID#1501, DOLocationID#1502, coalesce(payment_type#1503, cast(5 as int)) AS payment_type#1572, fare_amount#1504, extra#1505, mta_tax#1506, tip_amount#1507, tolls_amount#1508, improvement_surcharge#1509, total_amount#1510, congestion_surcharge#1511, airport_fee#1512]\n",
      "                                          +- Filter atleastnnonnulls(1, VendorId#1494, tpep_pickup_datetime#1495, tpep_dropoff_datetime#1496, passenger_count#1497, trip_distance#1498, RatecodeID#1499, store_and_fwd_flag#1500, PULocationID#1501, DOLocationID#1502, payment_type#1503, fare_amount#1504, extra#1505, mta_tax#1506, tip_amount#1507, tolls_amount#1508, improvement_surcharge#1509, total_amount#1510, congestion_surcharge#1511, airport_fee#1512)\n",
      "                                             +- Filter (trip_distance#1498 > 0.0)\n",
      "                                                +- Filter (passenger_count#1497 > cast(0 as double))\n",
      "                                                   +- Relation [VendorId#1494,tpep_pickup_datetime#1495,tpep_dropoff_datetime#1496,passenger_count#1497,trip_distance#1498,RatecodeID#1499,store_and_fwd_flag#1500,PULocationID#1501,DOLocationID#1502,payment_type#1503,fare_amount#1504,extra#1505,mta_tax#1506,tip_amount#1507,tolls_amount#1508,improvement_surcharge#1509,total_amount#1510,congestion_surcharge#1511,airport_fee#1512] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [DOLocationID#1502, improvement_surcharge#1509, tpep_dropoff_datetime#1496, PULocationID#1501, trip_distance#1498, tolls_amount#1508, RatecodeID#1571, VendorId#1494, tip_amount#1507, payment_type#1572, fare_amount#1504, passenger_count#1497, store_and_fwd_flag#1500, extra#1505, airport_fee#1512, congestion_surcharge#1511, total_amount#1510, tpep_pickup_datetime#1495, mta_tax#1506], [VendorID#1494, cast(passenger_count#1497 as int) AS PassengerCount#1832, trip_distance#1498 AS TripDistance#1820, tpep_pickup_datetime#1495 AS PickupTime#1843, tpep_dropoff_datetime#1496 AS DropTime#1854, PUlocationID#1501 AS PickupLocationId#1865, DOlocationID#1502 AS DropLocationId#1876, RatecodeID#1571, total_amount#1510 AS TotalAmount#1887, payment_type#1572 AS PaymentType#1898, year(cast(tpep_pickup_datetime#1495 as date)) AS TripYear#1931, month(cast(tpep_pickup_datetime#1495 as date)) AS TripMonth#1943, dayofmonth(cast(tpep_pickup_datetime#1495 as date)) AS TripDay#1944, round((cast((unix_timestamp(tpep_dropoff_datetime#1496, yyyy-MM-dd HH:mm:ss, Some(America/Argentina/Buenos_Aires), false) - unix_timestamp(tpep_pickup_datetime#1495, yyyy-MM-dd HH:mm:ss, Some(America/Argentina/Buenos_Aires), false)) as double) / 60.0), 0) AS TripTimeInMinutes#1973]\n",
      "+- Project [VendorId#1494, tpep_pickup_datetime#1495, tpep_dropoff_datetime#1496, passenger_count#1497, trip_distance#1498, coalesce(nanvl(RatecodeID#1499, null), 1.0) AS RatecodeID#1571, store_and_fwd_flag#1500, PULocationID#1501, DOLocationID#1502, coalesce(payment_type#1503, 5) AS payment_type#1572, fare_amount#1504, extra#1505, mta_tax#1506, tip_amount#1507, tolls_amount#1508, improvement_surcharge#1509, total_amount#1510, congestion_surcharge#1511, airport_fee#1512]\n",
      "   +- Filter ((((isnotnull(passenger_count#1497) AND isnotnull(trip_distance#1498)) AND isnotnull(tpep_pickup_datetime#1495)) AND isnotnull(tpep_dropoff_datetime#1496)) AND (((passenger_count#1497 > 0.0) AND (trip_distance#1498 > 0.0)) AND ((atleastnnonnulls(1, VendorId#1494, tpep_pickup_datetime#1495, tpep_dropoff_datetime#1496, passenger_count#1497, trip_distance#1498, RatecodeID#1499, store_and_fwd_flag#1500, PULocationID#1501, DOLocationID#1502, payment_type#1503, fare_amount#1504, extra#1505, mta_tax#1506, tip_amount#1507, tolls_amount#1508, improvement_surcharge#1509, total_amount#1510, congestion_surcharge#1511, airport_fee#1512) AND (tpep_pickup_datetime#1495 >= 2022-10-01 00:00:00)) AND (tpep_dropoff_datetime#1496 < 2022-11-01 00:00:00))))\n",
      "      +- Relation [VendorId#1494,tpep_pickup_datetime#1495,tpep_dropoff_datetime#1496,passenger_count#1497,trip_distance#1498,RatecodeID#1499,store_and_fwd_flag#1500,PULocationID#1501,DOLocationID#1502,payment_type#1503,fare_amount#1504,extra#1505,mta_tax#1506,tip_amount#1507,tolls_amount#1508,improvement_surcharge#1509,total_amount#1510,congestion_surcharge#1511,airport_fee#1512] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DOLocationID#1502, improvement_surcharge#1509, tpep_dropoff_datetime#1496, PULocationID#1501, trip_distance#1498, tolls_amount#1508, RatecodeID#1571, VendorId#1494, tip_amount#1507, payment_type#1572, fare_amount#1504, passenger_count#1497, store_and_fwd_flag#1500, extra#1505, airport_fee#1512, congestion_surcharge#1511, total_amount#1510, tpep_pickup_datetime#1495, mta_tax#1506], functions=[], output=[VendorId#1494, PassengerCount#1832, TripDistance#1820, PickupTime#1843, DropTime#1854, PickupLocationId#1865, DropLocationId#1876, RatecodeID#1571, TotalAmount#1887, PaymentType#1898, TripYear#1931, TripMonth#1943, TripDay#1944, TripTimeInMinutes#1973])\n",
      "+- Exchange hashpartitioning(DOLocationID#1502, improvement_surcharge#1509, tpep_dropoff_datetime#1496, PULocationID#1501, trip_distance#1498, tolls_amount#1508, RatecodeID#1571, VendorId#1494, tip_amount#1507, payment_type#1572, fare_amount#1504, passenger_count#1497, store_and_fwd_flag#1500, extra#1505, airport_fee#1512, congestion_surcharge#1511, total_amount#1510, tpep_pickup_datetime#1495, mta_tax#1506, 200), ENSURE_REQUIREMENTS, [plan_id=878]\n",
      "   +- *(1) HashAggregate(keys=[DOLocationID#1502, knownfloatingpointnormalized(normalizenanandzero(improvement_surcharge#1509)) AS improvement_surcharge#1509, tpep_dropoff_datetime#1496, PULocationID#1501, knownfloatingpointnormalized(normalizenanandzero(trip_distance#1498)) AS trip_distance#1498, knownfloatingpointnormalized(normalizenanandzero(tolls_amount#1508)) AS tolls_amount#1508, knownfloatingpointnormalized(normalizenanandzero(RatecodeID#1571)) AS RatecodeID#1571, VendorId#1494, knownfloatingpointnormalized(normalizenanandzero(tip_amount#1507)) AS tip_amount#1507, payment_type#1572, knownfloatingpointnormalized(normalizenanandzero(fare_amount#1504)) AS fare_amount#1504, knownfloatingpointnormalized(normalizenanandzero(passenger_count#1497)) AS passenger_count#1497, store_and_fwd_flag#1500, knownfloatingpointnormalized(normalizenanandzero(extra#1505)) AS extra#1505, knownfloatingpointnormalized(normalizenanandzero(airport_fee#1512)) AS airport_fee#1512, knownfloatingpointnormalized(normalizenanandzero(congestion_surcharge#1511)) AS congestion_surcharge#1511, knownfloatingpointnormalized(normalizenanandzero(total_amount#1510)) AS total_amount#1510, tpep_pickup_datetime#1495, knownfloatingpointnormalized(normalizenanandzero(mta_tax#1506)) AS mta_tax#1506], functions=[], output=[DOLocationID#1502, improvement_surcharge#1509, tpep_dropoff_datetime#1496, PULocationID#1501, trip_distance#1498, tolls_amount#1508, RatecodeID#1571, VendorId#1494, tip_amount#1507, payment_type#1572, fare_amount#1504, passenger_count#1497, store_and_fwd_flag#1500, extra#1505, airport_fee#1512, congestion_surcharge#1511, total_amount#1510, tpep_pickup_datetime#1495, mta_tax#1506])\n",
      "      +- *(1) Project [VendorId#1494, tpep_pickup_datetime#1495, tpep_dropoff_datetime#1496, passenger_count#1497, trip_distance#1498, coalesce(nanvl(RatecodeID#1499, null), 1.0) AS RatecodeID#1571, store_and_fwd_flag#1500, PULocationID#1501, DOLocationID#1502, coalesce(payment_type#1503, 5) AS payment_type#1572, fare_amount#1504, extra#1505, mta_tax#1506, tip_amount#1507, tolls_amount#1508, improvement_surcharge#1509, total_amount#1510, congestion_surcharge#1511, airport_fee#1512]\n",
      "         +- *(1) Filter ((((((((isnotnull(passenger_count#1497) AND isnotnull(trip_distance#1498)) AND isnotnull(tpep_pickup_datetime#1495)) AND isnotnull(tpep_dropoff_datetime#1496)) AND (passenger_count#1497 > 0.0)) AND (trip_distance#1498 > 0.0)) AND atleastnnonnulls(1, VendorId#1494, tpep_pickup_datetime#1495, tpep_dropoff_datetime#1496, passenger_count#1497, trip_distance#1498, RatecodeID#1499, store_and_fwd_flag#1500, PULocationID#1501, DOLocationID#1502, payment_type#1503, fare_amount#1504, extra#1505, mta_tax#1506, tip_amount#1507, tolls_amount#1508, improvement_surcharge#1509, total_amount#1510, congestion_surcharge#1511, airport_fee#1512)) AND (tpep_pickup_datetime#1495 >= 2022-10-01 00:00:00)) AND (tpep_dropoff_datetime#1496 < 2022-11-01 00:00:00))\n",
      "            +- FileScan csv [VendorId#1494,tpep_pickup_datetime#1495,tpep_dropoff_datetime#1496,passenger_count#1497,trip_distance#1498,RatecodeID#1499,store_and_fwd_flag#1500,PULocationID#1501,DOLocationID#1502,payment_type#1503,fare_amount#1504,extra#1505,mta_tax#1506,tip_amount#1507,tolls_amount#1508,improvement_surcharge#1509,total_amount#1510,congestion_surcharge#1511,airport_fee#1512] Batched: false, DataFilters: [isnotnull(passenger_count#1497), isnotnull(trip_distance#1498), isnotnull(tpep_pickup_datetime#1..., Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/emi/Escritorio/curso_spark/datos/YellowTaxis_202210.csv], PartitionFilters: [], PushedFilters: [IsNotNull(passenger_count), IsNotNull(trip_distance), IsNotNull(tpep_pickup_datetime), IsNotNull..., ReadSchema: struct<VendorId:int,tpep_pickup_datetime:timestamp,tpep_dropoff_datetime:timestamp,passenger_coun...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yellowTaxiDF.explain( mode = \"extended\" )\n",
    "\n",
    "# Other modes - simple, codegen, cost, formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a6255149",
   "metadata": {
    "metadata": {},
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 19:53:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:53:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:53:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:53:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:53:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:53:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:53:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:53:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 71:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+------------+-------------------+-------------------+----------------+--------------+-----------+-----------+--------+---------+-------+-----------------+--------+\n",
      "|VendorID|PassengerCount|TripDistance|         PickupTime|           DropTime|PickupLocationId|DropLocationId|TotalAmount|PaymentType|TripYear|TripMonth|TripDay|TripTimeInMinutes|TripType|\n",
      "+--------+--------------+------------+-------------------+-------------------+----------------+--------------+-----------+-----------+--------+---------+-------+-----------------+--------+\n",
      "|       2|             2|         1.8|2022-10-01 00:18:28|2022-10-01 00:26:54|              48|            68|      14.76|          1|    2022|       10|      1|              8.0|SoloTrip|\n",
      "|       2|             2|        1.41|2022-10-01 00:40:53|2022-10-01 00:47:01|             211|           261|      12.36|          1|    2022|       10|      1|              6.0|SoloTrip|\n",
      "|       2|             1|        1.54|2022-10-01 00:07:52|2022-10-01 00:15:34|              79|           170|      13.56|          1|    2022|       10|      1|              8.0|SoloTrip|\n",
      "|       2|             1|       12.38|2022-10-01 00:35:27|2022-10-01 01:03:59|             231|           159|      48.36|          1|    2022|       10|      1|             29.0|SoloTrip|\n",
      "|       2|             1|        1.14|2022-10-01 01:00:26|2022-10-01 01:05:14|             231|           158|      12.74|          1|    2022|       10|      1|              5.0|SoloTrip|\n",
      "|       2|             1|        3.59|2022-10-01 00:34:47|2022-10-01 00:45:59|             107|            66|       17.3|          1|    2022|       10|      1|             11.0|SoloTrip|\n",
      "|       2|             1|        4.87|2022-10-01 00:44:13|2022-10-01 00:56:28|              79|            13|      23.76|          1|    2022|       10|      1|             12.0|SoloTrip|\n",
      "|       2|             1|        3.25|2022-10-01 01:07:45|2022-10-01 01:24:43|             148|           246|      23.14|          1|    2022|       10|      1|             17.0|SoloTrip|\n",
      "|       2|             2|        2.43|2022-10-01 01:18:27|2022-10-01 01:27:21|               4|           233|      15.36|          1|    2022|       10|      1|              9.0|SoloTrip|\n",
      "|       1|             1|         5.7|2022-10-01 01:14:18|2022-10-01 01:34:54|             246|           116|       20.8|          2|    2022|       10|      1|             21.0|SoloTrip|\n",
      "|       2|             1|        3.04|2022-10-01 01:43:32|2022-10-01 01:55:12|              68|            79|      18.36|          1|    2022|       10|      1|             12.0|SoloTrip|\n",
      "|       2|             1|         3.4|2022-10-01 01:37:49|2022-10-01 01:49:15|             239|           164|       16.8|          1|    2022|       10|      1|             11.0|SoloTrip|\n",
      "|       2|             1|        0.66|2022-10-01 01:06:43|2022-10-01 01:08:55|             233|           170|       9.36|          1|    2022|       10|      1|              2.0|SoloTrip|\n",
      "|       2|             4|        6.41|2022-10-01 01:47:43|2022-10-01 02:11:39|             211|             7|       30.8|          2|    2022|       10|      1|             24.0|SoloTrip|\n",
      "|       1|             1|         2.9|2022-10-01 02:27:22|2022-10-01 02:38:41|             143|            75|       16.3|          1|    2022|       10|      1|             11.0|SoloTrip|\n",
      "|       1|             2|        18.3|2022-10-01 02:45:30|2022-10-01 03:17:21|             132|           230|       75.1|          1|    2022|       10|      1|             32.0|SoloTrip|\n",
      "|       2|             6|        1.13|2022-10-01 03:28:52|2022-10-01 03:35:21|              48|           161|      11.16|          1|    2022|       10|      1|              6.0|SoloTrip|\n",
      "|       2|             1|        1.52|2022-10-01 03:47:43|2022-10-01 03:57:07|             100|           162|       11.3|          2|    2022|       10|      1|              9.0|SoloTrip|\n",
      "|       2|             6|        2.58|2022-10-01 03:38:18|2022-10-01 03:45:48|             107|           141|      14.14|          1|    2022|       10|      1|              8.0|SoloTrip|\n",
      "|       2|             1|        0.77|2022-10-01 03:54:12|2022-10-01 03:57:09|             230|            48|        7.8|          2|    2022|       10|      1|              3.0|SoloTrip|\n",
      "+--------+--------------+------------+-------------------+-------------------+----------------+--------------+-----------+-----------+--------+---------+-------+-----------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# NOT IN VIDEO, RUN AND SEE PLAN IN SPARK\n",
    "\n",
    "yellowTaxiDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646aac39",
   "metadata": {},
   "source": [
    "### Read JSON file using schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e71867b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxiBasesSchema = (\n",
    "                    StructType\n",
    "                    ([\n",
    "                        StructField(\"License Number\"         , StringType()    , True),\n",
    "                        StructField(\"Entity Name\"            , StringType()    , True),\n",
    "                        StructField(\"Telephone Number\"       , LongType()      , True),\n",
    "                        StructField(\"SHL Endorsed\"           , StringType()    , True),\n",
    "                        StructField(\"Type of Base\"           , StringType()    , True),\n",
    "\n",
    "                        StructField(\"Address\", \n",
    "                                        StructType\n",
    "                                        ([\n",
    "                                            StructField(\"Building\"   , StringType(),   True),\n",
    "                                            StructField(\"Street\"     , StringType(),   True), \n",
    "                                            StructField(\"City\"       , StringType(),   True), \n",
    "                                            StructField(\"State\"      , StringType(),   True), \n",
    "                                            StructField(\"Postcode\"   , StringType(),   True)\n",
    "                                        ]),\n",
    "                                    True\n",
    "                                   ),\n",
    "                        \n",
    "                        StructField(\"GeoLocation\", \n",
    "                                        StructType\n",
    "                                        ([\n",
    "                                            StructField(\"Latitude\"   , StringType(),   True),\n",
    "                                            StructField(\"Longitude\"  , StringType(),   True), \n",
    "                                            StructField(\"Location\"   , StringType(),   True)\n",
    "                                        ]),\n",
    "                                    True\n",
    "                                   )  \n",
    "                  ])\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7f747588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------------------------+----------------+------------+---------------------------+-----------------------------------------------------------+------------------------------------------------+\n",
      "|License Number|Entity Name                           |Telephone Number|SHL Endorsed|Type of Base               |Address                                                    |GeoLocation                                     |\n",
      "+--------------+--------------------------------------+----------------+------------+---------------------------+-----------------------------------------------------------+------------------------------------------------+\n",
      "|B02865        |VIER-NY,LLC                           |6466657536      |No          |BLACK CAR BASE             |{636, WEST   28 STREET, NEW YORK, NY, 10001}               |{40.75273, -74.006408, (40.75273, -74.006408)}  |\n",
      "|B02634        |VETERANS RADIO DISPATCHER CORP.       |7183647878      |No          |LIVERY BASE                |{131, KINGSBRIDGE ROAD, BRONX, NY, 10468}                  |{40.86927, -73.90281, (40.86927, -73.90281)}    |\n",
      "|B80094        |ALPHA VAN LINE                        |5162850750      |No          |COMMUTER VAN AUTHORITY BASE|{115-54, 238 STREET, ELMONT, NY, 11003}                    |{40.693473, -73.724446, (40.693473, -73.724446)}|\n",
      "|B02677        |A.T.B. CAR AND LIMOUSINE SERVICE, INC.|7184854444      |No          |LIVERY BASE                |{866, NEW LOTS AVENUE, BROOKLYN, NY, 11208}                |{40.667838, -73.8788, (40.667838, -73.8788)}    |\n",
      "|B02152        |KYOEI LIMOUSINE, INC.                 |7183263258      |No          |LUXURY/LIMOUSINE           |{57-48, MASPETH AVENUE, MASPETH, NY, 11378}                |{40.722961, -73.91031, (40.722961, -73.91031)}  |\n",
      "|B02844        |ENDOR CAR & DRIVER,LLC.               |4154758459      |No          |BLACK CAR BASE             |{31-00, 47 AVENUE  SUITE # 4123A, LIC, NY, 11101}          |{40.742082, -73.93552, (40.742082, -73.93552)}  |\n",
      "|B02841        |SKYWAY EXECUTIVE SERVICE, INC         |7183595959      |No          |BLACK CAR BASE             |{68-20A, FRESH MEADOW LANE, FRESH MEADOWS, NY, 11365}      |{40.733337, -73.794706, (40.733337, -73.794706)}|\n",
      "|B00472        |FARRELL'S LEASING CO.                 |2128616300      |No          |LUXURY/LIMOUSINE           |{22-11, 38 AVENUE, LIC, NY, 11101}                         |{40.757077, -73.937504, (40.757077, -73.937504)}|\n",
      "|B01739        |CITY CAR SERVICE CORP                 |7184182222      |No          |LIVERY BASE                |{429, SUTTER AVENUE, BROOKLYN, NY, 11212}                  |{40.668473, -73.903383, (40.668473, -73.903383)}|\n",
      "|B00248        |YELLOWSTONE TRANSPORTATION INC.       |7185397777      |No          |LIVERY BASE                |{41-31, MAIN STREET, FLUSHING, NY, 11355}                  |{40.758114, -73.82962, (40.758114, -73.82962)}  |\n",
      "|B02861        |ELTRI CAR SERVICE INC                 |7183820100      |No          |BLACK CAR BASE             |{1811, EAST    7 STREET, BROOKLYN, NY, 11223}              |{40.612302, -73.963221, (40.612302, -73.963221)}|\n",
      "|B02093        |V.J. CAR & LIMO SVC INC.              |7186417777      |No          |LIVERY BASE                |{104-08, 123RD STREET, RICHMOND HILL, NY, 11419}           |{40.691906, -73.82342, (40.691906, -73.82342)}  |\n",
      "|B02980        |ADVANTAGE LIMO OF NY INC              |3478655876      |No          |LUXURY/LIMOUSINE           |{6701, BAY PARKWAY  OFFICE #1 3RD FLR, BROOKLYN, NY, 11204}|{40.603784, -73.972558, (40.603784, -73.972558)}|\n",
      "|B02878        |ELF-NY,LLC                            |6466657540      |No          |BLACK CAR BASE             |{636, WEST   28 STREET, NEW YORK, NY, 10001}               |{40.75273, -74.00641, (40.75273, -74.00641)}    |\n",
      "|B90691        |ALTA MEDICAL TRANSPORTATION, INC.     |7188972582      |No          |PARATRANSIT BASE           |{85-02, 67 AVENUE  SUITE 1, REGO PARK, NY, 11374}          |{40.717275, -73.86211, (40.717275, -73.86211)}  |\n",
      "|B02840        |RAELEEN CAR CORP.                     |9176031815      |No          |BLACK CAR BASE             |{565, VAN NEST AVENUE, BRONX, NY, 10460}                   |{40.755734, -73.93795, (40.755734, -73.93795)}  |\n",
      "|B02658        |ASCONA CAR SERVICE INC.               |7186461611      |No          |LIVERY BASE                |{2421, BATH AVENUE, BROOKLYN, NY, 11214}                   |{40.595422, -73.992368, (40.595422, -73.992368)}|\n",
      "|B02331        |CHICO EXPRESS LIMO, INC.              |7182367777      |No          |BLACK CAR BASE             |{6318, 14 AVENUE, BROOKLYN, NY, 11219}                     |{40.625609, -73.998684, (40.625609, -73.998684)}|\n",
      "|B02969        |LYFE AUTO GROUP INC                   |7187066234      |No          |BLACK CAR BASE             |{32-10, GREENPOINT AVENUE, LIC, NY, 11101}                 |{40.734944, -73.935733, (40.734944, -73.935733)}|\n",
      "|B02651        |SAROS TRANSPORTATION GROUP INC.       |7185640084      |No          |BLACK CAR BASE             |{626, REXCORP PLAZA, UNIONDALE, NY, 11556}                 |{40.638931, -73.982954, (40.638931, -73.982954)}|\n",
      "+--------------+--------------------------------------+----------------+------------+---------------------------+-----------------------------------------------------------+------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxiBasesDF = (\n",
    "                  spark\n",
    "                    .read    \n",
    "                    .option(\"multiline\", \"true\")\n",
    "    \n",
    "                    .schema(taxiBasesSchema)\n",
    "    \n",
    "                    .json(\"datos/TaxiBases.json\")\n",
    "              )\n",
    "\n",
    "taxiBasesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c509df1d",
   "metadata": {},
   "source": [
    "### Extract nested fields from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9fc6e7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+---------------+--------------------+-------------+------------+---------------+-----------+------------+\n",
      "|BaseLicenseNumber|          EntityName|AddressBuilding|       AddressStreet|  AddressCity|AddressState|AddressPostCode|GeoLatitude|GeoLongitude|\n",
      "+-----------------+--------------------+---------------+--------------------+-------------+------------+---------------+-----------+------------+\n",
      "|           B02865|         VIER-NY,LLC|            636|    WEST   28 STREET|     NEW YORK|          NY|          10001|   40.75273|  -74.006408|\n",
      "|           B02634|VETERANS RADIO DI...|            131|    KINGSBRIDGE ROAD|        BRONX|          NY|          10468|   40.86927|   -73.90281|\n",
      "|           B80094|      ALPHA VAN LINE|         115-54|          238 STREET|       ELMONT|          NY|          11003|  40.693473|  -73.724446|\n",
      "|           B02677|A.T.B. CAR AND LI...|            866|     NEW LOTS AVENUE|     BROOKLYN|          NY|          11208|  40.667838|    -73.8788|\n",
      "|           B02152|KYOEI LIMOUSINE, ...|          57-48|      MASPETH AVENUE|      MASPETH|          NY|          11378|  40.722961|   -73.91031|\n",
      "|           B02844|ENDOR CAR & DRIVE...|          31-00|47 AVENUE  SUITE ...|          LIC|          NY|          11101|  40.742082|   -73.93552|\n",
      "|           B02841|SKYWAY EXECUTIVE ...|         68-20A|   FRESH MEADOW LANE|FRESH MEADOWS|          NY|          11365|  40.733337|  -73.794706|\n",
      "|           B00472|FARRELL'S LEASING...|          22-11|           38 AVENUE|          LIC|          NY|          11101|  40.757077|  -73.937504|\n",
      "|           B01739|CITY CAR SERVICE ...|            429|       SUTTER AVENUE|     BROOKLYN|          NY|          11212|  40.668473|  -73.903383|\n",
      "|           B00248|YELLOWSTONE TRANS...|          41-31|         MAIN STREET|     FLUSHING|          NY|          11355|  40.758114|   -73.82962|\n",
      "|           B02861|ELTRI CAR SERVICE...|           1811|    EAST    7 STREET|     BROOKLYN|          NY|          11223|  40.612302|  -73.963221|\n",
      "|           B02093|V.J. CAR & LIMO S...|         104-08|        123RD STREET|RICHMOND HILL|          NY|          11419|  40.691906|   -73.82342|\n",
      "|           B02980|ADVANTAGE LIMO OF...|           6701|BAY PARKWAY  OFFI...|     BROOKLYN|          NY|          11204|  40.603784|  -73.972558|\n",
      "|           B02878|          ELF-NY,LLC|            636|    WEST   28 STREET|     NEW YORK|          NY|          10001|   40.75273|   -74.00641|\n",
      "|           B90691|ALTA MEDICAL TRAN...|          85-02|  67 AVENUE  SUITE 1|    REGO PARK|          NY|          11374|  40.717275|   -73.86211|\n",
      "|           B02840|   RAELEEN CAR CORP.|            565|     VAN NEST AVENUE|        BRONX|          NY|          10460|  40.755734|   -73.93795|\n",
      "|           B02658|ASCONA CAR SERVIC...|           2421|         BATH AVENUE|     BROOKLYN|          NY|          11214|  40.595422|  -73.992368|\n",
      "|           B02331|CHICO EXPRESS LIM...|           6318|           14 AVENUE|     BROOKLYN|          NY|          11219|  40.625609|  -73.998684|\n",
      "|           B02969| LYFE AUTO GROUP INC|          32-10|   GREENPOINT AVENUE|          LIC|          NY|          11101|  40.734944|  -73.935733|\n",
      "|           B02651|SAROS TRANSPORTAT...|            626|       REXCORP PLAZA|    UNIONDALE|          NY|          11556|  40.638931|  -73.982954|\n",
      "+-----------------+--------------------+---------------+--------------------+-------------+------------+---------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxiBasesFlatDF = (\n",
    "                        taxiBasesDF\n",
    "                            .select(\n",
    "                                      col(\"License Number\").alias(\"BaseLicenseNumber\"),\n",
    "                                      col(\"Entity Name\").alias(\"EntityName\"),\n",
    "\n",
    "                                      col(\"Address.Building\").alias(\"AddressBuilding\"),\n",
    "\n",
    "                                      col(\"Address.Street\").alias(\"AddressStreet\"),\n",
    "                                      col(\"Address.City\").alias(\"AddressCity\"),\n",
    "                                      col(\"Address.State\").alias(\"AddressState\"),\n",
    "                                      col(\"Address.Postcode\").alias(\"AddressPostCode\"),\n",
    "\n",
    "                                      col(\"GeoLocation.Latitude\").alias(\"GeoLatitude\"),\n",
    "                                      col(\"GeoLocation.Longitude\").alias(\"GeoLongitude\")\n",
    "                                   )\n",
    "                  )\n",
    "\n",
    "taxiBasesFlatDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1606e8",
   "metadata": {},
   "source": [
    "### Aggregate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1a4925",
   "metadata": {},
   "source": [
    "Crear un nuevo DataFrame llamado yellowTaxiDFReport que contendrá un informe generado a partir del DataFrame yellowTaxiDF\n",
    "\n",
    "El informe se generará agrupando por las columnas \"PickupLocationId\" y \"DropLocationId\"\n",
    "y se realizarán cálculos en las columnas \"TripTimeInMinutes\" y \"TotalAmount\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "696a36e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se importa la función avg y sum de pyspark.sql.functions para realizar los cálculos de promedio y suma respectivamente\n",
    "from pyspark.sql.functions import avg, sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a689d8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 19:58:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:58:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:58:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:58:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:58:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:58:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:58:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 19:58:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+------------------+------------------+\n",
      "|PickupLocationId|DropLocationId|       AvgTripTime|         SumAmount|\n",
      "+----------------+--------------+------------------+------------------+\n",
      "|             265|           182|              17.0|              25.0|\n",
      "|             265|           230|              37.0|            383.34|\n",
      "|             265|            86|23.666666666666668|              68.8|\n",
      "|             265|           116|              40.0|             53.16|\n",
      "|             265|           170|              40.0|350.43999999999994|\n",
      "|             265|           231|              10.0|              15.3|\n",
      "|             265|           233|              45.0|             78.85|\n",
      "|             265|           218|              23.0|              45.0|\n",
      "|             265|           132|              31.9|            598.71|\n",
      "|             265|            10|              11.0|             14.55|\n",
      "|             265|            23|               2.0|              41.6|\n",
      "|             265|           144|              17.0|              16.8|\n",
      "|             265|           107|              27.5|            101.59|\n",
      "|             265|            28|              19.0|              58.0|\n",
      "|             265|           135|              22.6|             291.0|\n",
      "|             265|            39|              24.0|              37.3|\n",
      "|             265|           114|              60.0|             220.8|\n",
      "|             265|           194|              38.0|             88.08|\n",
      "|             265|           139|              19.0|              20.0|\n",
      "|             265|           203|              41.0|             130.8|\n",
      "+----------------+--------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "yellowTaxiDFReport = (\n",
    "    yellowTaxiDF\n",
    "        .groupBy(\"PickupLocationId\", \"DropLocationId\")\n",
    "        .agg(\n",
    "            # Se calcula el promedio de la columna \"TripTimeInMinutes\" y se crea una nueva columna \"AvgTripTime\"\n",
    "            avg(\"TripTimeInMinutes\").alias(\"AvgTripTime\"),\n",
    "            # Se calcula la suma de la columna \"TotalAmount\" y se crea una nueva columna \"SumAmount\"\n",
    "            sum(\"TotalAmount\").alias(\"SumAmount\")\n",
    "        )\n",
    "        # Se ordena el DataFrame por la columna \"PickupLocationId\" en orden descendente\n",
    "        .orderBy(col(\"PickupLocationId\").desc())\n",
    ")\n",
    "\n",
    "# Se muestra el DataFrame resultante\n",
    "yellowTaxiDFReport.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8097f6",
   "metadata": {},
   "source": [
    "#### Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d7beb6",
   "metadata": {},
   "source": [
    "\n",
    "1. Based on PickupTime, add a new column with day text = Monday to Sunday\n",
    "\n",
    "2. Based on PickupTime, add a new column with month text = January to December\n",
    "\n",
    "3. Based on PickupTime, add a new column with value being the last day of month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e1dd401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, date_format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "61a9b999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mdate_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'ColumnOrName'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Converts a date/timestamp/string to a value of string in the format specified by the date\n",
      "format given by the second argument.\n",
      "\n",
      "A pattern could be for instance `dd.MM.yyyy` and could return a string like '18.03.1993'. All\n",
      "pattern letters of `datetime pattern`_. can be used.\n",
      "\n",
      ".. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "\n",
      ".. versionadded:: 1.5.0\n",
      "\n",
      ".. versionchanged:: 3.4.0\n",
      "    Supports Spark Connect.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Whenever possible, use specialized functions like `year`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "date : :class:`~pyspark.sql.Column` or str\n",
      "    input column of values to format.\n",
      "format: str\n",
      "    format to use to represent datetime values.\n",
      "\n",
      "Returns\n",
      "-------\n",
      ":class:`~pyspark.sql.Column`\n",
      "    string value representing formatted datetime.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      ">>> df.select(date_format('dt', 'MM/dd/yyy').alias('date')).collect()\n",
      "[Row(date='04/08/2015')]\n",
      "\u001b[0;31mFile:\u001b[0m      ~/Escritorio/curso_spark/env/lib/python3.8/site-packages/pyspark/sql/functions.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "date_format?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3b008d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 20:03:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:03:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:04:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:04:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:04:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:04:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:04:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:04:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 78:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+------------+-------------------+-------------------+----------------+--------------+-----------+-----------+--------+---------+-------+-----------------+--------+----------+\n",
      "|VendorID|PassengerCount|TripDistance|         PickupTime|           DropTime|PickupLocationId|DropLocationId|TotalAmount|PaymentType|TripYear|TripMonth|TripDay|TripTimeInMinutes|TripType|PickupDate|\n",
      "+--------+--------------+------------+-------------------+-------------------+----------------+--------------+-----------+-----------+--------+---------+-------+-----------------+--------+----------+\n",
      "|       2|             2|         1.8|2022-10-01 00:18:28|2022-10-01 00:26:54|              48|            68|      14.76|          1|    2022|       10|      1|              8.0|SoloTrip|2022-10-01|\n",
      "|       2|             2|        1.41|2022-10-01 00:40:53|2022-10-01 00:47:01|             211|           261|      12.36|          1|    2022|       10|      1|              6.0|SoloTrip|2022-10-01|\n",
      "+--------+--------------+------------+-------------------+-------------------+----------------+--------------+-----------+-----------+--------+---------+-------+-----------------+--------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Convert PickupTime column to date type\n",
    "df = yellowTaxiDF.withColumn(\"PickupDate\", col(\"PickupTime\").cast(\"date\"))\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5447b52",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
    "\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Symbol</th>\n",
    "      <th>Meaning</th>\n",
    "      <th>Presentation</th>\n",
    "      <th>Examples</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td><strong>G</strong></td>\n",
    "      <td>era</td>\n",
    "      <td>text</td>\n",
    "      <td>AD; Anno Domini</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>y</strong></td>\n",
    "      <td>year</td>\n",
    "      <td>year</td>\n",
    "      <td>2020; 20</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>D</strong></td>\n",
    "      <td>day-of-year</td>\n",
    "      <td>number(3)</td>\n",
    "      <td>189</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>M/L</strong></td>\n",
    "      <td>month-of-year</td>\n",
    "      <td>month</td>\n",
    "      <td>7; 07; Jul; July</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>d</strong></td>\n",
    "      <td>day-of-month</td>\n",
    "      <td>number(2)</td>\n",
    "      <td>28</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>Q/q</strong></td>\n",
    "      <td>quarter-of-year</td>\n",
    "      <td>number/text</td>\n",
    "      <td>3; 03; Q3; 3rd quarter</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>E</strong></td>\n",
    "      <td>day-of-week</td>\n",
    "      <td>text</td>\n",
    "      <td>Tue; Tuesday</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>F</strong></td>\n",
    "      <td>aligned day of week in month</td>\n",
    "      <td>number(1)</td>\n",
    "      <td>3</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>a</strong></td>\n",
    "      <td>am-pm-of-day</td>\n",
    "      <td>am-pm</td>\n",
    "      <td>PM</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>h</strong></td>\n",
    "      <td>clock-hour-of-am-pm (1-12)</td>\n",
    "      <td>number(2)</td>\n",
    "      <td>12</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>K</strong></td>\n",
    "      <td>hour-of-am-pm (0-11)</td>\n",
    "      <td>number(2)</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>k</strong></td>\n",
    "      <td>clock-hour-of-day (1-24)</td>\n",
    "      <td>number(2)</td>\n",
    "      <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>H</strong></td>\n",
    "      <td>hour-of-day (0-23)</td>\n",
    "      <td>number(2)</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>m</strong></td>\n",
    "      <td>minute-of-hour</td>\n",
    "      <td>number(2)</td>\n",
    "      <td>30</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>s</strong></td>\n",
    "      <td>second-of-minute</td>\n",
    "      <td>number(2)</td>\n",
    "      <td>55</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>S</strong></td>\n",
    "      <td>fraction-of-second</td>\n",
    "      <td>fraction</td>\n",
    "      <td>978</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>V</strong></td>\n",
    "      <td>time-zone ID</td>\n",
    "      <td>zone-id</td>\n",
    "      <td>America/Los_Angeles; Z; -08:30</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>z</strong></td>\n",
    "      <td>time-zone name</td>\n",
    "      <td>zone-name</td>\n",
    "      <td>Pacific Standard Time; PST</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>O</strong></td>\n",
    "      <td>localized zone-offset</td>\n",
    "      <td>offset-O</td>\n",
    "      <td>GMT+8; GMT+08:00; UTC-08:00;</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>X</strong></td>\n",
    "      <td>zone-offset ‘Z’ for zero</td>\n",
    "      <td>offset-X</td>\n",
    "      <td>Z; -08; -0830; -08:30; -083015; -08:30:15;</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>x</strong></td>\n",
    "      <td>zone-offset</td>\n",
    "      <td>offset-x</td>\n",
    "      <td>+0000; -08; -0830; -08:30; -083015; -08:30:15;</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>Z</strong></td>\n",
    "      <td>zone-offset</td>\n",
    "      <td>offset-Z</td>\n",
    "      <td>+0000; -0800; -08:00;</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>‘</strong></td>\n",
    "      <td>escape for text</td>\n",
    "      <td>delimiter</td>\n",
    "      <td>&nbsp;</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>’‘</strong></td>\n",
    "      <td>single quote</td>\n",
    "      <td>literal</td>\n",
    "      <td>’</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>[</strong></td>\n",
    "      <td>optional section start</td>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>]</strong></td>\n",
    "      <td>optional section end</td>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0de93691",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 20:10:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:10:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:10:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:10:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:10:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:10:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:10:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:10:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 82:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+------------+-------------------+-------------------+----------------+--------------+-----------+-----------+--------+---------+-------+-----------------+--------+----------+--------+\n",
      "|VendorID|PassengerCount|TripDistance|         PickupTime|           DropTime|PickupLocationId|DropLocationId|TotalAmount|PaymentType|TripYear|TripMonth|TripDay|TripTimeInMinutes|TripType|PickupDate| DayText|\n",
      "+--------+--------------+------------+-------------------+-------------------+----------------+--------------+-----------+-----------+--------+---------+-------+-----------------+--------+----------+--------+\n",
      "|       2|             2|         1.8|2022-10-01 00:18:28|2022-10-01 00:26:54|              48|            68|      14.76|          1|    2022|       10|      1|              8.0|SoloTrip|2022-10-01|Saturday|\n",
      "|       2|             2|        1.41|2022-10-01 00:40:53|2022-10-01 00:47:01|             211|           261|      12.36|          1|    2022|       10|      1|              6.0|SoloTrip|2022-10-01|Saturday|\n",
      "+--------+--------------+------------+-------------------+-------------------+----------------+--------------+-----------+-----------+--------+---------+-------+-----------------+--------+----------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract the day of the week and map it to the corresponding day text\n",
    "df = df.withColumn(\"DayText\", date_format(col(\"PickupDate\"), \"EEEE\"))\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fd6db403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 20:13:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:13:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:13:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:13:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:13:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:13:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:13:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:13:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 84:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "| DayText|\n",
      "+--------+\n",
      "|Saturday|\n",
      "|Saturday|\n",
      "+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df[\"DayText\",].show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e206af59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 20:21:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:21:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:21:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:21:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:21:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:21:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:21:48 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:21:48 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 86:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "| DayText|MonthText|\n",
      "+--------+---------+\n",
      "|Saturday|  October|\n",
      "|Saturday|  October|\n",
      "+--------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Exercise 1: Answer\n",
    "df = df.withColumn(\"MonthText\", date_format(col(\"PickupDate\"), \"MMMM\"))\n",
    "df[\"DayText\",\"MonthText\"].show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "65fa75df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import  last_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "af69ca32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mlast_day\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'ColumnOrName'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Returns the last day of the month which the given date belongs to.\n",
      "\n",
      ".. versionadded:: 1.5.0\n",
      "\n",
      ".. versionchanged:: 3.4.0\n",
      "    Supports Spark Connect.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "date : :class:`~pyspark.sql.Column` or str\n",
      "    target column to compute on.\n",
      "\n",
      "Returns\n",
      "-------\n",
      ":class:`~pyspark.sql.Column`\n",
      "    last day of the month.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> df = spark.createDataFrame([('1997-02-10',)], ['d'])\n",
      ">>> df.select(last_day(df.d).alias('date')).collect()\n",
      "[Row(date=datetime.date(1997, 2, 28))]\n",
      "\u001b[0;31mFile:\u001b[0m      ~/Escritorio/curso_spark/env/lib/python3.8/site-packages/pyspark/sql/functions.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "last_day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f8717a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 20:30:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:30:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:30:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:30:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:30:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:30:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:30:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:30:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 88:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----------+\n",
      "| DayText|MonthText|   LastDay|\n",
      "+--------+---------+----------+\n",
      "|Saturday|  October|2022-10-31|\n",
      "|Saturday|  October|2022-10-31|\n",
      "|Saturday|  October|2022-10-31|\n",
      "|Saturday|  October|2022-10-31|\n",
      "|Saturday|  October|2022-10-31|\n",
      "|Saturday|  October|2022-10-31|\n",
      "|Saturday|  October|2022-10-31|\n",
      "|Saturday|  October|2022-10-31|\n",
      "|Saturday|  October|2022-10-31|\n",
      "|Saturday|  October|2022-10-31|\n",
      "|Saturday|  October|2022-10-31|\n",
      "|Saturday|  October|2022-10-31|\n",
      "|Saturday|  October|2022-10-31|\n",
      "|Saturday|  October|2022-10-31|\n",
      "|Saturday|  October|2022-10-31|\n",
      "|Saturday|  October|2022-10-31|\n",
      "|Saturday|  October|2022-10-31|\n",
      "|Saturday|  October|2022-10-31|\n",
      "|Saturday|  October|2022-10-31|\n",
      "|Saturday|  October|2022-10-31|\n",
      "+--------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"LastDay\", last_day(col(\"PickupDate\")))\n",
    "df[\"DayText\",\"MonthText\",\"LastDay\"].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "41e603ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce number of DataFrame partitions to 4\n",
    "\n",
    "yellowTaxiDF = yellowTaxiDF.coalesce(4)\n",
    "\n",
    "yellowTaxiDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4338d3de",
   "metadata": {},
   "source": [
    "## Save data in CSV format to storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6d14c638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 20:37:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:37:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:37:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:37:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:37:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:37:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:37:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:37:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    yellowTaxiDF    \n",
    "            .write\n",
    "            \n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"dateFormat\", \"yyyy-MM-dd HH:mm:ss.S\")\n",
    "    \n",
    "            .mode(\"overwrite\")    # Options - Append, ErrorIfExists, Ignore, Overwrite\n",
    "    \n",
    "            .csv(\"datos/Output/YellowTaxisOutput\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6e2ff915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6ba874b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['part-00002-4edf6df5-262f-40f8-b4df-98bf88e49790-c000.csv',\n",
       " '_SUCCESS',\n",
       " 'part-00001-4edf6df5-262f-40f8-b4df-98bf88e49790-c000.csv',\n",
       " '._SUCCESS.crc',\n",
       " '.part-00000-4edf6df5-262f-40f8-b4df-98bf88e49790-c000.csv.crc',\n",
       " '.part-00002-4edf6df5-262f-40f8-b4df-98bf88e49790-c000.csv.crc',\n",
       " '.part-00003-4edf6df5-262f-40f8-b4df-98bf88e49790-c000.csv.crc',\n",
       " 'part-00003-4edf6df5-262f-40f8-b4df-98bf88e49790-c000.csv',\n",
       " 'part-00000-4edf6df5-262f-40f8-b4df-98bf88e49790-c000.csv',\n",
       " '.part-00001-4edf6df5-262f-40f8-b4df-98bf88e49790-c000.csv.crc']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listdir(\"datos/Output/YellowTaxisOutput\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df3fe38",
   "metadata": {},
   "source": [
    "### Save data in Parquet format to storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "70e3ea82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 20:40:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:40:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:40:16 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:40:16 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:40:16 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:40:16 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:40:16 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:40:16 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    yellowTaxiDF\n",
    "            .write\n",
    "    \n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"dateFormat\", \"yyyy-MM-dd HH:mm:ss.S\")\n",
    "    \n",
    "            .mode(\"overwrite\")\n",
    "    \n",
    "            .parquet(\"datos/Output/YellowTaxisOutput.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4b071622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['part-00002-8a0caac1-1150-42a2-a36d-71c471802110-c000.snappy.parquet',\n",
       " '.part-00003-8a0caac1-1150-42a2-a36d-71c471802110-c000.snappy.parquet.crc',\n",
       " 'part-00000-8a0caac1-1150-42a2-a36d-71c471802110-c000.snappy.parquet',\n",
       " 'part-00003-8a0caac1-1150-42a2-a36d-71c471802110-c000.snappy.parquet',\n",
       " '_SUCCESS',\n",
       " '.part-00002-8a0caac1-1150-42a2-a36d-71c471802110-c000.snappy.parquet.crc',\n",
       " '._SUCCESS.crc',\n",
       " '.part-00000-8a0caac1-1150-42a2-a36d-71c471802110-c000.snappy.parquet.crc',\n",
       " 'part-00001-8a0caac1-1150-42a2-a36d-71c471802110-c000.snappy.parquet',\n",
       " '.part-00001-8a0caac1-1150-42a2-a36d-71c471802110-c000.snappy.parquet.crc']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listdir(\"datos/Output/YellowTaxisOutput.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8717f072",
   "metadata": {},
   "source": [
    "### Save partitioned data in Parquet format to storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "033a65a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 20:42:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:42:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:42:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:42:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:42:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:42:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:42:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:42:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:42:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:42:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:42:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/11 20:42:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    yellowTaxiDF\n",
    "            .write\n",
    "    \n",
    "            .partitionBy(\"VendorId\")\n",
    "            \n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"dateFormat\", \"yyyy-MM-dd HH:mm:ss.S\")\n",
    "    \n",
    "            .mode(\"overwrite\")\n",
    "    \n",
    "            .parquet(\"datos/Output/YellowTaxisPartitionedOutput.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9f6cddb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorId=2', '_SUCCESS', 'VendorId=1', '._SUCCESS.crc']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listdir(\"datos/Output/YellowTaxisPartitionedOutput.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37a14af",
   "metadata": {},
   "source": [
    "### Check performance between non-partitioned & partitioned datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eef425a",
   "metadata": {},
   "source": [
    "#### 1. Performance check: Run query on non-partitioned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7112d860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+\n",
      "|PickupLocationId|  avg(TotalAmount)|\n",
      "+----------------+------------------+\n",
      "|             148|20.027366458264947|\n",
      "|             243| 25.97614035087719|\n",
      "|              31| 26.21666666666667|\n",
      "|             137|17.343399488927055|\n",
      "|              85| 27.50609756097561|\n",
      "|             251|            61.825|\n",
      "|              65| 25.43594555873929|\n",
      "|             255| 22.50868589743589|\n",
      "|              53| 39.64958333333334|\n",
      "|             133|33.552083333333336|\n",
      "|              78| 34.10000000000001|\n",
      "|             108| 49.70862068965517|\n",
      "|             155|            38.944|\n",
      "|             193| 19.06381578947367|\n",
      "|             211|18.900272379269484|\n",
      "|              34|28.052105263157895|\n",
      "|             101|        36.5390625|\n",
      "|             126| 26.52592592592593|\n",
      "|              81| 37.83181818181818|\n",
      "|              28| 35.15681818181818|\n",
      "+----------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run query on non-partitioned dataset\n",
    "\n",
    "outputDF = (\n",
    "                spark\n",
    "                    .read\n",
    "                    .parquet(\"datos/Output/YellowTaxisOutput.parquet\")\n",
    "    \n",
    "                    .where(\"VendorId = 1\")\n",
    "    \n",
    "                    .groupBy(\"PickupLocationId\")\n",
    "                    .agg(avg(\"TotalAmount\"))\n",
    "           )\n",
    "\n",
    "outputDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96bc448",
   "metadata": {},
   "source": [
    "#### 2. Performance check: Run query on partitioned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9f186c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+\n",
      "|PickupLocationId|  avg(TotalAmount)|\n",
      "+----------------+------------------+\n",
      "|             148|20.027366458264964|\n",
      "|             243|25.976140350877188|\n",
      "|              31| 26.21666666666667|\n",
      "|             137| 17.34339948892702|\n",
      "|              85|27.506097560975615|\n",
      "|             251|            61.825|\n",
      "|              65|25.435945558739288|\n",
      "|             255| 22.50868589743589|\n",
      "|              53| 39.64958333333333|\n",
      "|             133|33.552083333333336|\n",
      "|              78|              34.1|\n",
      "|             155|            38.944|\n",
      "|             108| 49.70862068965517|\n",
      "|             211|18.900272379269484|\n",
      "|             193| 19.06381578947367|\n",
      "|              34|28.052105263157895|\n",
      "|             126| 26.52592592592593|\n",
      "|             101|        36.5390625|\n",
      "|              81| 37.83181818181818|\n",
      "|              28| 35.15681818181818|\n",
      "+----------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputDF = (\n",
    "                spark\n",
    "                    .read\n",
    "                    .parquet(\"datos/Output/YellowTaxisPartitionedOutput.parquet\")\n",
    "    \n",
    "                    .where(\"VendorId = 1\")\n",
    "    \n",
    "                    .groupBy(\"PickupLocationId\")\n",
    "                    .agg(avg(\"TotalAmount\"))\n",
    "           )\n",
    "\n",
    "outputDF.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
